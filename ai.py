import ast
import filecmp
from multiprocessing import Pool
import time
import pandas as pd
import numpy as np
import json
import os
import matplotlib.pyplot as plt

import seaborn as sns

# sns color palette
COLOR_PALETTE = "hls"
sns.set_palette(COLOR_PALETTE)


from matplotlib.colors import ListedColormap
from matplotlib.collections import LineCollection


from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier
from sklearn.feature_selection import SelectKBest, chi2, f_classif
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import (
    accuracy_score,
    classification_report,
    confusion_matrix,
    f1_score,
    precision_score,
    recall_score,
)

from sklearn.svm import SVC

from cosco import (
    runCOSCO,
    NUM_SIM_STEPS,
    HOSTS,
    CONTAINERS,
    ROUTER_BW,
    INTERVAL_TIME,
    NEW_CONTAINERS,
    FAULT_RATE,
    FAULT_TIME,
    FAULT_INCREASE_TIME,
    RECOVER_TIME,
    FAULTY_HOSTS,
    FAILURE_TYPES,
    ACCUMULATIVE_FAULTS,
)

# from cosco import runCOSCO, NUM_SIM_STEPS, FAULT_INCREASE_TIME, FAULTY_HOSTS, ACCUMULATIVE_FAULTS

# FAULT_RATE = 0.3
# FAULT_TIME = 6
# RECOVER_TIME = 18


hosts_str = "".join([str(i) for i in FAULTY_HOSTS])
type_str = "acc" if ACCUMULATIVE_FAULTS else "rec"
fault_type_str = "".join([str(i[0]).lower() for i in FAILURE_TYPES])

DATAPATH = f"AI/backups/{NUM_SIM_STEPS}i_{FAULT_RATE}fr_{FAULT_TIME}ft_{RECOVER_TIME}rt_{FAULT_INCREASE_TIME}fit_hosts{hosts_str}_{type_str}_{fault_type_str}/"
FIGURES_PATH = f"{DATAPATH}/figures/"
CSV_PATH = f"logs/MyFog_MyAzure2019Workload_{NUM_SIM_STEPS}_{HOSTS}_{CONTAINERS}_{ROUTER_BW}_{INTERVAL_TIME}_{NEW_CONTAINERS}/hostinfo_with_interval.csv"

NUMBER_OF_SIMULATIONS = 30

NUMBER_OF_REPETITIONS = 50


def generate_datasets():
    """
    Generates datasets for the AI by calling the COSCO simulator
    Will generate NUMBER_OF_SIMULATIONS datasets

    """
    # create datapath folder if it doesn't exist

    os.makedirs(os.path.dirname(DATAPATH), exist_ok=True)
    os.makedirs(os.path.dirname(DATAPATH + "data/"), exist_ok=True)
    os.makedirs(os.path.dirname(FIGURES_PATH), exist_ok=True)
    os.makedirs(os.path.dirname(FIGURES_PATH + "analysis/"), exist_ok=True)
    os.makedirs(os.path.dirname(FIGURES_PATH + "metrics/"), exist_ok=True)

    # old version (without multiprocessing)
    for i in range(NUMBER_OF_SIMULATIONS):
        datapath_i = f"{DATAPATH}data/data{i}.csv"
        # skip if log file already exists
        if os.path.isfile(datapath_i):
            continue

        print(f"Creating DATA {i+1} of {NUMBER_OF_SIMULATIONS}")
        # run simulation
        runCOSCO(prints=False)

        # copy log file to datapath
        os.system(f"cp {CSV_PATH} {datapath_i}")

    """
    # create pool of processes
    with Pool(processes=NUMBER_OF_SIMULATIONS) as p:
        p.map(call_cosco, range(NUMBER_OF_SIMULATIONS))


    # verify if all files are different
    for i in range(NUMBER_OF_SIMULATIONS):
        for j in range(i+1, NUMBER_OF_SIMULATIONS):
            datapath_i = f"{DATAPATH}data/data{i}.csv"
            datapath_j = f"{DATAPATH}data/data{j}.csv"
            if filecmp.cmp(datapath_i, datapath_j):
                print(f"Files {i+1} and {j+1} are equal. Deleting {i+1} and re-generating.")
                os.remove(datapath_i)
                call_cosco(i)
    """


def call_cosco(i):
    """
    Calls the COSCO simulator and saves the log file to the datapath (dataset)

    Parameters
    ----------
    i : int
        The index of the dataset to be generated
    """

    datapath_i = f"{DATAPATH}data/data{i}.csv"
    # skip if log file already exists
    if os.path.isfile(datapath_i):
        return

    print(f"Creating DATA {i+1} of {NUMBER_OF_SIMULATIONS}")
    # run simulation
    runCOSCO(prints=False)

    # copy log file to datapath
    # check if file exists
    if os.path.isfile(CSV_PATH):
        os.system(f"cp {CSV_PATH} {datapath_i}")


def evaluate_datasets(failure="cpu"):
    """
    Evaluates the datasets generated by the COSCO simulator



    """
    # EVALUATING DATA
    metrics_1 = [[], [], [], []]  # accuracy, precision, recall, f1
    metrics_all = [[], [], [], []]  # accuracy, precision, recall, f1
    metrics_12_3 = [[], [], [], []]  # accuracy, precision, recall, f1

    best_f1 = 0
    best_pred = None
    best_cpu = None

    metrics_path = FIGURES_PATH + "metrics/"

    for i in range(NUMBER_OF_SIMULATIONS):
        print(f"Evaluating DATA {i+1} of {NUMBER_OF_SIMULATIONS}")

        datapath_i = f"{DATAPATH}data/data{i}.csv"

        # read data
        data = pd.read_csv(datapath_i)

        # data has lists on each column. in this case, we want only the first element of each list
        # remove some columns that are not needed for now

        headers = [
            "interval",
            "cpu",
            "numcontainers",
            "baseips",
            "ipsavailable",
            "ipscap",
            "apparentips",
        ]

        data = data[headers + "cpufailures"]

        # create copies of data
        host1 = data.copy()
        host2 = data.copy()
        host3 = data.copy()

        for header in headers:
            host1[header] = host1[header].apply(lambda x: json.loads(x)[0])
            host2[header] = host2[header].apply(lambda x: json.loads(x)[1])
            host3[header] = host3[header].apply(lambda x: json.loads(x)[2])

        # count failures
        # print("Class distribution:")
        # print("Host1:\n", host1['cpufailures'].value_counts())
        # print("Host2:\n", host2['cpufailures'].value_counts())
        # print("Host3:\n", host3['cpufailures'].value_counts())

        # WORK WITH BINARY CLASSIFICATION
        host1["cpufailures"] = host1["cpufailures"].apply(lambda x: 1 if x > 0 else 0)
        host2["cpufailures"] = host2["cpufailures"].apply(lambda x: 1 if x > 0 else 0)
        host3["cpufailures"] = host3["cpufailures"].apply(lambda x: 1 if x > 0 else 0)

        # print("Class distribution after binary classification:")
        # print("Host1:\n", host1['cpufailures'].value_counts())
        # print("Host2:\n", host2['cpufailures'].value_counts())
        # print("Host3:\n", host3['cpufailures'].value_counts())

        # TRAIN AND EVALUATE ONLY ON HOST1
        metrics_temp, _ = train_and_evaluate(
            host1,
            "cpufailures",
            RandomForestClassifier(n_estimators=100, n_jobs=-1),
            binary=True,
        )
        metrics_1[0].extend(metrics_temp[0])
        metrics_1[1].extend(metrics_temp[1])
        metrics_1[2].extend(metrics_temp[2])
        metrics_1[3].extend(metrics_temp[3])

        # TRAIN AND EVALUATE ON ALL HOSTS TOGETHER

        # concatenate data
        all_hosts = pd.concat([host1, host2, host3])

        metrics_temp, _ = train_and_evaluate(
            all_hosts,
            "cpufailures",
            RandomForestClassifier(n_estimators=100, n_jobs=-1),
            binary=True,
        )
        metrics_all[0].extend(metrics_temp[0])
        metrics_all[1].extend(metrics_temp[1])
        metrics_all[2].extend(metrics_temp[2])
        metrics_all[3].extend(metrics_temp[3])
        # TRAIN ON HOSTS 1 AND 2, EVALUATE ON HOST 3

        # concatenate data
        host1_2 = pd.concat([host1, host2])

        metrics_temp, best_info = train_and_evaluate(
            host1_2,
            "cpufailures",
            RandomForestClassifier(n_estimators=100, n_jobs=-1),
            data_test=host3,
            binary=True,
        )
        metrics_12_3[0].extend(metrics_temp[0])
        metrics_12_3[1].extend(metrics_temp[1])
        metrics_12_3[2].extend(metrics_temp[2])
        metrics_12_3[3].extend(metrics_temp[3])

        if best_info[1] > best_f1:
            best_f1 = best_info[1]
            best_pred = best_info[0]
            best_cpu = host3["cpu"].values

    # plot histograms for f1 scores
    plt.figure()
    plt.hist(metrics_1[3], bins=10, alpha=0.5)
    plt.hist(metrics_all[3], bins=10, alpha=0.5)
    plt.hist(metrics_12_3[3], bins=10, alpha=0.5)
    plt.legend(["Host1", "All hosts", "Host1 and Host2"])
    plt.savefig(f"{metrics_path}/f1_scores.png")
    plt.savefig(f"{metrics_path}/f1_scores.svg")

    # plots for host1
    plot_metrics(metrics_1, "metrics_1")

    # plots for all hosts
    plot_metrics(metrics_all, "metrics_all")

    # plots for train on host1 and host2, test on host3
    plot_metrics(metrics_12_3, "metrics_12_3")

    # plot cpu usage with the color of the confusion label
    classes = []
    for i in range(len(best_pred)):
        if best_pred[i] == best_cpu.iloc[:, -1].values[i]:
            if best_pred[i] == 1:
                # True Positive
                classes.append(1)
            else:
                # True Negative
                classes.append(0)
        else:
            if best_pred[i] == 1:
                # False Positive
                classes.append(3)
            else:
                # False Negative
                classes.append(2)

    colors = ListedColormap(["blue", "green", "yellow", "orange"])

    plt.figure(figsize=(15, 5))
    scatter = plt.scatter(range(len(best_pred)), best_cpu, c=classes, cmap=colors)
    plt.xlabel("Time")
    plt.ylabel("CPU usage")
    plt.legend(
        handles=scatter.legend_elements()[0],
        loc="upper left",
        labels=["True Negative", "True Positive", "False Negative", "False Positive"],
    )

    plt.savefig(f"{metrics_path}/cpu_12_3.png")
    plt.savefig(f"{metrics_path}/cpu_12_3.svg")


def train_and_evaluate_big_data():
    data_temp = pd.read_csv(f"{DATAPATH}data/data0.csv")

    num_hosts = int(len(json.loads(data_temp["cpu"][0])) / 2)

    # create big data dataframe
    big_data = [pd.DataFrame() for _ in range(num_hosts)]

    for i in range(NUMBER_OF_SIMULATIONS):
        datapath_i = f"{DATAPATH}data/data{i}.csv"
        data_temp = pd.read_csv(datapath_i)
        # print(f'Number of hosts: {num_hosts}')

        data_temp = data_temp.drop(
            columns=["interval", "ram", "ramavailable", "disk", "diskavailable"]
        )
        # get headers
        headers = data_temp.columns

        # create list of copies of data
        data = [data_temp.copy() for _ in range(num_hosts)]

        for j in range(num_hosts):
            for header in headers:
                data[j][header] = data[j][header].apply(lambda x: json.loads(x)[j])

            # append data to big data
            big_data[j] = big_data[j].append(data[j])

    for i in range(num_hosts):
        big_data[i] = big_data[i].reset_index(drop=True)

    # train and evaluate
    metrics = [[], [], [], []]
    for _ in range(NUMBER_OF_REPETITIONS):
        # split data
        train, test = train_test_split(big_data[0], test_size=0.2)

        # train model
        clf = RandomForestClassifier(n_estimators=100)
        clf.fit(train.iloc[:, :-1], train.iloc[:, -1])

        # predict
        pred = clf.predict(test.iloc[:, :-1])

        # evaluate
        metrics[0].append(accuracy_score(test.iloc[:, -1], pred))
        metrics[1].append(precision_score(test.iloc[:, -1], pred, average="macro"))
        metrics[2].append(recall_score(test.iloc[:, -1], pred, average="macro"))
        metrics[3].append(
            0
            if metrics[1][-1] * metrics[2][-1] == 0
            else 2
            * (metrics[1][-1] * metrics[2][-1])
            / (metrics[1][-1] + metrics[2][-1])
        )

    # plot metrics
    plt.figure()
    plt.boxplot(metrics)
    plt.xticks([1, 2, 3, 4], ["Accuracy", "Precision", "Recall", "F1"])
    plt.savefig(f"{FIGURES_PATH}/metrics_big_data.png")
    plt.savefig(f"{FIGURES_PATH}/metrics_big_data.svg")

    # train in all data
    merged_big_data = pd.DataFrame()
    for i in range(num_hosts):
        merged_big_data = merged_big_data.append(big_data[i])

    merged_big_data = merged_big_data.reset_index(drop=True)
    print(merged_big_data.shape)

    # split data
    train, test = train_test_split(merged_big_data, test_size=0.2)

    # train model
    clf = RandomForestClassifier(n_estimators=100)
    clf.fit(train.iloc[:, :-1], train.iloc[:, -1])

    # predict
    pred = clf.predict(test.iloc[:, :-1])

    # evaluate
    metrics = []
    metrics.append(accuracy_score(test.iloc[:, -1], pred))
    metrics.append(precision_score(test.iloc[:, -1], pred, average="macro"))
    metrics.append(recall_score(test.iloc[:, -1], pred, average="macro"))
    metrics.append(
        0
        if metrics[1] * metrics[2] == 0
        else 2 * (metrics[1] * metrics[2]) / (metrics[1] + metrics[2])
    )

    print(metrics)


def plot_distribution(data, dataset_index):
    num_hosts = len(data)

    # limit color palette to number of hosts
    colors = sns.color_palette(COLOR_PALETTE, num_hosts)

    # count number of cpu failures
    counts_cpu = [list(host["cpufailures"].value_counts()) for host in data]

    # count number of ram failures
    counts_ram = [list(host["ramfailures"].value_counts()) for host in data]

    num_max_labels = max(
        [
            max([len(count)] for count in counts_cpu)[0],
            max([len(count) for count in counts_ram]),
        ]
    )
    # print(f'Number of labels: {num_max_labels}')

    for count in counts_cpu:
        while len(count) < num_max_labels:
            count.append(0)

    for count in counts_ram:
        while len(count) < num_max_labels:
            count.append(0)

    plt.figure()
    fig, ax = plt.subplots(figsize=(10, 5), tight_layout=True)

    x = np.arange(num_max_labels)
    x_labels = [str(label) for label in range(num_max_labels)]

    width = 1 / ((num_hosts * 2) + 1)
    multiplier = 0

    for h_i in range(num_hosts):
        offset = width * multiplier
        multiplier += 1

        # cpu failures
        rects = ax.bar(
            x + offset,
            counts_cpu[h_i],
            width,
            label=f"Host {h_i}",
            color=colors[h_i],
        )

        for rect in rects:
            height = rect.get_height()

            if height > 0:
                ax.annotate(
                    f"{height}",
                    xy=(rect.get_x() + rect.get_width() / 2, height),
                    xytext=(0, 3),
                    textcoords="offset points",
                    ha="center",
                    va="bottom",
                )

        offset = width * (multiplier + num_hosts - 1)

        # ram failures
        rects = ax.bar(
            x + offset, counts_ram[h_i], width, hatch="///", color=colors[h_i]
        )

        for rect in rects:
            height = rect.get_height()

            if height > 0:
                ax.annotate(
                    f"{height}",
                    xy=(rect.get_x() + rect.get_width() / 2, height),
                    xytext=(0, 3),
                    textcoords="offset points",
                    ha="center",
                    va="bottom",
                )

    ax.set_xlabel("Failure Intensity")
    ax.set_ylabel("Number of Occurrences")

    ax.set_xticks(x + (num_hosts * 2 - 1) / 2 * width)
    ax.set_xticklabels(x_labels)

    # add invisible data to add second legend
    ax.bar(1, 0, color="gray", label="CPU")
    ax.bar(1, 0, color="gray", hatch="///", label="RAM")

    ax.legend(loc="upper right")

    plt.savefig(
        f"{FIGURES_PATH}analysis/individuals/data{dataset_index}/png/failure_distribution.png"
    )
    plt.savefig(
        f"{FIGURES_PATH}analysis/individuals/data{dataset_index}/svg/failure_distribution.svg"
    )


def plot_cpu_ram(data, dataset_index):
    num_hosts = int(len(data)/2)
    individual_data_path = f"{FIGURES_PATH}analysis/individuals/data{dataset_index}/"

    def plot_usage_and_failures(component):
        component_failures = f"{component}failures"
        
        fig_hos, ax_hos = plt.subplots(
            nrows=num_hosts, ncols=1, sharex=True, sharey=True, figsize=(10, 10)
        )

        fig_all, ax_all = plt.subplots(
            nrows=num_hosts * 2, ncols=1, sharex=True, sharey=True, figsize=(10, 15)
        )

        # each row represents a different host
        # x = ["interval"]
        # y = [component]
        # color intervals according to failure intensity [component_failures]

        most_failures = 0
        most_failures_index = 0

        scs_hos = [None for _ in range(num_hosts)]

        for h_i in range(num_hosts * 2):
            # print(f'Host {h_i}')
            # print(f'{component.upper()}: {data[h_i][component]}')
            # print(f'{component.upper()} Failures: {data[h_i][component_failures]}')

            ### INDIVIDUAL ###
            fig_ind, ax_ind = plt.subplots(figsize=(10, 5))
            ax_ind.plot(
                data[h_i]["interval"],
                data[h_i][component],
                color="black",
                label=f"{component.upper()} Usage",
            )

            if h_i < num_hosts:
                # component failures
                sc = ax_ind.scatter(
                    data[h_i]["interval"],
                    data[h_i][component],
                    c=data[h_i][component_failures],
                    cmap="magma_r",
                )

                fig_ind.legend(
                    *sc.legend_elements(),
                    bbox_to_anchor=(1.13, 0.62),
                    title="Failure Intensity",
                )

            ax_ind.set_xlabel("Interval")
            ax_ind.set_ylabel(f"{component.upper()} Usage (%)")

            ax_ind.set_xlim([0, len(data[h_i]["interval"])])
            # ax_ind.set_ylim([0, 100])

            fig_ind.legend(*ax_ind.get_legend_handles_labels(), bbox_to_anchor=(1.13, 0.72))

            fig_ind.tight_layout()
            fig_ind.savefig(f"{individual_data_path}png/indiv/{component}_{h_i}.png")
            fig_ind.savefig(f"{individual_data_path}svg/indiv/{component}_{h_i}.svg")

            ### HOST PLOTS ###
            if h_i < num_hosts:
                if max(data[h_i][component_failures]) > most_failures:
                    most_failures = max(data[h_i][component_failures])
                    most_failures_index = h_i

                # component usage
                ax_hos[h_i].plot(
                    data[h_i]["interval"],
                    data[h_i][component],
                    color="black",
                    label=f"{component.upper()} Usage",
                )

                # component failures
                scs_hos[h_i] = ax_hos[h_i].scatter(
                    data[h_i]["interval"],
                    data[h_i][component],
                    c=data[h_i][component_failures],
                    cmap="magma_r",
                )

                #ax_hos[h_i].set_ylim([0, 100])
                ax_hos[h_i].set_xlim([0, len(data[h_i]["interval"])])
                ax_hos[h_i].set_title(f"Host {h_i}")

            ### ALL PLOTS (with replicas) ###
            # component usage
            ax_all[h_i].plot(
                data[h_i]["interval"],
                data[h_i][component],
                color="black",
                label=f"{component.upper()} Usage",
            )

            if h_i < num_hosts:

                # component failures
                ax_all[h_i].scatter(
                    data[h_i]["interval"],
                    data[h_i][component],
                    c=data[h_i][component_failures],
                    cmap="magma_r",
                )

            #ax_all[h_i].set_ylim([0, 100])
            ax_all[h_i].set_xlim([0, len(data[h_i]["interval"])])

        # HOST PLOTS
        # xlabel and ylabel in the middle
        fig_hos.supylabel(f"{component.upper()} Usage (%)")
        fig_hos.supxlabel("Interval")


        fig_hos.legend(*ax_hos[0].get_legend_handles_labels(), bbox_to_anchor=(1.13, 0.72))
        fig_hos.legend(
            *scs_hos[most_failures_index].legend_elements(),
            bbox_to_anchor=(1.13, 0.62),
            title="Failure Intensity",
        )

        fig_hos.tight_layout()
        fig_hos.savefig(f"{individual_data_path}png/{component}.png")
        fig_hos.savefig(f"{individual_data_path}png/{component}.svg")

        # ALL PLOTS
        # xlabel and ylabel in the middle
        fig_all.supylabel(f"{component.upper()} Usage (%)")
        fig_all.supxlabel("Interval")


        fig_all.legend(*ax_all[0].get_legend_handles_labels(), bbox_to_anchor=(1.13, 0.72))
        fig_all.legend(
            *scs_hos[most_failures_index].legend_elements(),
            bbox_to_anchor=(1.13, 0.62),
            title="Failure Intensity",
        )

        fig_all.tight_layout()
        fig_all.savefig(f"{individual_data_path}png/{component}_all.png")
        fig_all.savefig(f"{individual_data_path}png/{component}_all.svg")

    plot_usage_and_failures("cpu")
    plot_usage_and_failures("ram")


def plot_data():
    os.makedirs(os.path.dirname(FIGURES_PATH + "analysis/"), exist_ok=True)
    individual_path = FIGURES_PATH + "analysis/individuals/"
    os.makedirs(os.path.dirname(individual_path), exist_ok=True)

    for i in range(NUMBER_OF_SIMULATIONS):
        os.makedirs(os.path.dirname(f"{individual_path}data{i}/"), exist_ok=True)
        os.makedirs(os.path.dirname(f"{individual_path}data{i}/png/"), exist_ok=True)
        os.makedirs(os.path.dirname(f"{individual_path}data{i}/png/indiv/"), exist_ok=True)
        os.makedirs(os.path.dirname(f"{individual_path}data{i}/svg/"), exist_ok=True)
        os.makedirs(os.path.dirname(f"{individual_path}data{i}/svg/indiv/"), exist_ok=True)

        datapath_i = f"{DATAPATH}data/data{i}.csv"
        data_temp = pd.read_csv(datapath_i)

        num_hosts = int(len(json.loads(data_temp["cpu"][0])) / 2)
        # print(f'Number of hosts: {num_hosts}')

        data_temp = data_temp.drop(columns=["disk", "diskavailable"])
        # get headers
        headers = data_temp.columns

        # create list of copies of data
        data = [data_temp.copy() for _ in range(2 * num_hosts)]

        for j in range(len(data)):
            for header in headers:
                if header != "interval":
                    data[j][header] = data[j][header].apply(lambda x: json.loads(x)[j])

        plot_distribution(
            data[:num_hosts],   # only hosts, not replicas
            i,
        )

        plot_cpu_ram(data, i)

        # plot number pf containers
        fig, ax = plt.subplots(
            nrows=num_hosts, ncols=1, sharex=True, sharey=True, figsize=(10, 5)
        )

        for h_i in range(num_hosts):
            # INDIVIDUAL PLOT
            fig_in, ax_in = plt.subplots(figsize=(10, 5))
            ax_in.plot(
                data[h_i]["interval"],
                data[h_i]["numcontainers"]
            )

            # SUBPLOT

            # component usage
            ax[h_i].plot(
                data[h_i]["interval"],
                data[h_i]["numcontainers"]
            )

            #ax[h_i].set_ylim([0, 100])
            ax[h_i].set_xlim([0, len(data[h_i]["interval"])])
            ax[h_i].set_title(f"Host {h_i}")

        # ylabel in the middle
        ax[np.floor(num_hosts / 2).astype(int)].set_ylabel(
            f"Number of Containers", loc="center"
        )
        ax[-1].set_xlabel("Interval")
        plt.tight_layout()
        plt.savefig(f"{individual_path}data{i}/numcontainers.png")
        plt.savefig(f"{individual_path}data{i}/numcontainers.svg")


def big_merged_data_eda():
    # Exploratory Data Analysis on the merged data

    big_analysis_path = FIGURES_PATH + "analysis/big_merged_data_eda/"
    os.makedirs(os.path.dirname(big_analysis_path), exist_ok=True)
    os.makedirs(os.path.dirname(big_analysis_path + "pairs/"), exist_ok=True)

    # load and merge data
    merged_big_data_cpu = pd.DataFrame()
    merged_big_data_ram = pd.DataFrame()

    cpu_headers = [
        "cpu",
        "numcontainers",
        "baseips",
        "ipsavailable",
        "ipscap",
        "apparentips",
        "cpufailures",
    ]
    ram_headers = [
        "ram",
        "numcontainers",
        "ram_s",
        "ram_r",
        "ram_w",
        "ramavailable_s",
        "ramavailable_r",
        "ramavailable_w",
        "ramfailures",
    ]

    for i in range(NUMBER_OF_SIMULATIONS):
        datapath_i = f"{DATAPATH}data/data{i}.csv"
        data_temp = pd.read_csv(datapath_i)

        num_hosts = int(len(json.loads(data_temp["cpu"][0])) / 2)
        # print(f'Number of hosts: {num_hosts}')

        data_temp_cpu = data_temp[cpu_headers]
        data_temp_ram = data_temp[ram_headers]

        # create list of copies of data
        data_cpu = [data_temp_cpu.copy() for _ in range(num_hosts)]
        data_ram = [data_temp_ram.copy() for _ in range(num_hosts)]

        for j in range(num_hosts):
            for header in cpu_headers:
                data_cpu[j][header] = data_cpu[j][header].apply(
                    lambda x: json.loads(x)[j]
                )

            for header in ram_headers:
                data_ram[j][header] = data_ram[j][header].apply(
                    lambda x: json.loads(x)[j]
                )

        for j in range(num_hosts):
            # data_cpu[j]['host_ltype'] = j
            # data_ram[j]['host_ltype'] = j

            merged_big_data_cpu = merged_big_data_cpu.append(data_cpu[j])
            merged_big_data_ram = merged_big_data_ram.append(data_ram[j])

    merged_big_data_cpu = merged_big_data_cpu.reset_index(drop=True)
    merged_big_data_ram = merged_big_data_ram.reset_index(drop=True)

    print(merged_big_data_cpu.shape, merged_big_data_ram.shape)

    # 0. Divide by 2 the number of failures (each failure level corresponds to 2 containers)
    merged_big_data_cpu["cpufailures"] = merged_big_data_cpu["cpufailures"] // 2
    merged_big_data_ram["ramfailures"] = merged_big_data_ram["ramfailures"] // 2

    # following https://www.digitalocean.com/community/tutorials/exploratory-data-analysis-python

    # 1. Basic Information

    print("\n---- INFO ----")
    print("CPU:\n", merged_big_data_cpu.info())
    print("\nRAM:\n", merged_big_data_ram.info())

    print("\n---- DESCRIPTION ----")
    print("CPU:\n", merged_big_data_cpu.describe())
    print("\nRAM:\n", merged_big_data_ram.describe())

    # 2. Duplicate Values

    print(
        f"\n---- DUPLICATES:\tCPU: {merged_big_data_cpu.duplicated().sum()}\tRAM: {merged_big_data_ram.duplicated().sum()}"
    )

    # 5. Missing Values
    print(
        f"\n---- MISSING VALUES ----\nCPU:\n{merged_big_data_cpu.isnull().sum()}\nRAM:\n{merged_big_data_ram.isnull().sum()}"
    )

    # """

    # 10. Correlation Matrix
    plt.figure()
    fig, ax = plt.subplots(figsize=(10, 9), tight_layout=True)
    corr = merged_big_data_cpu.corr()
    sns.heatmap(corr, annot=True, fmt=".3f", ax=ax)
    plt.savefig(f"{big_analysis_path}correlation_matrix_cpu.png")
    plt.savefig(f"{big_analysis_path}correlation_matrix_cpu.svg")

    plt.figure()
    fig, ax = plt.subplots(figsize=(10, 9), tight_layout=True)
    corr = merged_big_data_ram.corr()
    sns.heatmap(corr, annot=True, fmt=".3f", ax=ax)
    plt.savefig(f"{big_analysis_path}correlation_matrix_ram.png")
    plt.savefig(f"{big_analysis_path}correlation_matrix_ram.svg")

    # Correlation Matrix shows that there is no strong correlation between cpufailures and [numcontainers, baseips, ipsavailable, ipscap, host_ltype]
    # Whith this information, we will try to predict cpufailures using all the features and compare it to the results of using only the features that have a correlation with cpufailures
    #   wich are [cpu, apparentips]

    """

    # Train and Evaluate with all features - CPU
    metrics, _ = train_and_evaluate(
        merged_big_data_cpu,
        "cpufailures",
        RandomForestClassifier(n_estimators=100, n_jobs=-1),
        binary=False,
    )
    # binary classification
    metrics_bin, _ = train_and_evaluate(
        merged_big_data_cpu,
        "cpufailures",
        RandomForestClassifier(n_estimators=100, n_jobs=-1),
        binary=True,
    )

    print(
        f'''\t{'METRICS ALL FEATURES':<48}\t{'METRICS ALL FEATURES (binary)':<48}
        \t{'accuracy':<10}{'precision':<10}{'recall':<10}{'f1':<10}\t\t{'accuracy':<10}{'precision':<10}{'recall':<10}{'f1':<10}
        mean\t{np.mean(metrics[0]):<10.4f}{np.mean(metrics[1]):<10.4f}{np.mean(metrics[2]):<10.4f}{np.mean(metrics[3]):<10.4f}\t\t{np.mean(metrics_bin[0]):<10.4f}{np.mean(metrics_bin[1]):<10.4f}{np.mean(metrics_bin[2]):<10.4f}{np.mean(metrics_bin[3]):<10.4f}
        median\t{np.median(metrics[0]):<10.4f}{np.median(metrics[1]):<10.4f}{np.median(metrics[2]):<10.4f}{np.median(metrics[3]):<10.4f}\t\t{np.median(metrics_bin[0]):<10.4f}{np.median(metrics_bin[1]):<10.4f}{np.median(metrics_bin[2]):<10.4f}{np.median(metrics_bin[3]):<10.4f}
        std\t{np.std(metrics[0]):<10.4f}{np.std(metrics[1]):<10.4f}{np.std(metrics[2]):<10.4f}{np.std(metrics[3]):<10.4f}\t\t{np.std(metrics_bin[0]):<10.4f}{np.std(metrics_bin[1]):<10.4f}{np.std(metrics_bin[2]):<10.4f}{np.std(metrics_bin[3]):<10.4f}
        '''\
    )

    #   METRICS ALL FEATURES                                METRICS ALL FEATURES (binary)                   
    #           accuracy  precision recall    f1        		accuracy  precision recall    f1        
    #   mean	0.9318    0.7297    0.5922    0.6384    		0.9459    0.8306    0.6780    0.7466    
    #   median	0.9317    0.7359    0.5920    0.6388    		0.9460    0.8302    0.6773    0.7462    
    #   std	    0.0012    0.0240    0.0119    0.0145    		0.0010    0.0074    0.0069    0.0051    
       

    # Train and Evaluate with all features - RAM
    metrics, _ = train_and_evaluate(
        merged_big_data_ram,
        "ramfailures",
        RandomForestClassifier(n_estimators=100, n_jobs=-1),
        binary=False,
    )
    # binary classification
    metrics_bin, _ = train_and_evaluate(
        merged_big_data_ram,
        "ramfailures",
        RandomForestClassifier(n_estimators=100, n_jobs=-1),
        binary=True,
    )

    print(
        f'''\t{'METRICS ALL FEATURES':<48}\t{'METRICS ALL FEATURES (binary)':<48}
        \t{'accuracy':<10}{'precision':<10}{'recall':<10}{'f1':<10}\t\t{'accuracy':<10}{'precision':<10}{'recall':<10}{'f1':<10}
        mean\t{np.mean(metrics[0]):<10.4f}{np.mean(metrics[1]):<10.4f}{np.mean(metrics[2]):<10.4f}{np.mean(metrics[3]):<10.4f}\t\t{np.mean(metrics_bin[0]):<10.4f}{np.mean(metrics_bin[1]):<10.4f}{np.mean(metrics_bin[2]):<10.4f}{np.mean(metrics_bin[3]):<10.4f}
        median\t{np.median(metrics[0]):<10.4f}{np.median(metrics[1]):<10.4f}{np.median(metrics[2]):<10.4f}{np.median(metrics[3]):<10.4f}\t\t{np.median(metrics_bin[0]):<10.4f}{np.median(metrics_bin[1]):<10.4f}{np.median(metrics_bin[2]):<10.4f}{np.median(metrics_bin[3]):<10.4f}
        std\t{np.std(metrics[0]):<10.4f}{np.std(metrics[1]):<10.4f}{np.std(metrics[2]):<10.4f}{np.std(metrics[3]):<10.4f}\t\t{np.std(metrics_bin[0]):<10.4f}{np.std(metrics_bin[1]):<10.4f}{np.std(metrics_bin[2]):<10.4f}{np.std(metrics_bin[3]):<10.4f}
        '''\
    )

    #   METRICS ALL FEATURES                            	METRICS ALL FEATURES (binary)                   
    #   	    accuracy  precision recall    f1        		accuracy  precision recall    f1        
    #   mean	0.8801    0.2511    0.2500    0.2355    		0.8793    0.1341    0.0044    0.0085    
    #   median	0.8801    0.2457    0.2500    0.2353    		0.8794    0.1299    0.0042    0.0082    
    #   std	    0.0014    0.0180    0.0003    0.0006    		0.0015    0.0316    0.0011    0.0022    
        


    # Train and Evaluate with all features
    metrics, _ = train_and_evaluate(merged_big_data, 'cpufailures', RandomForestClassifier(n_estimators=100, n_jobs=-1), binary=False)
    # binary classification
    metrics_bin, _ = train_and_evaluate(merged_big_data, 'cpufailures', RandomForestClassifier(n_estimators=100, n_jobs=-1), binary=True)

    print(f'''\t{'METRICS ALL FEATURES':<48}\t{'METRICS ALL FEATURES (binary)':<48}
        \t{'accuracy':<10}{'precision':<10}{'recall':<10}{'f1':<10}\t\t{'accuracy':<10}{'precision':<10}{'recall':<10}{'f1':<10}
        mean\t{np.mean(metrics[0]):<10.4f}{np.mean(metrics[1]):<10.4f}{np.mean(metrics[2]):<10.4f}{np.mean(metrics[3]):<10.4f}\t\t{np.mean(metrics_bin[0]):<10.4f}{np.mean(metrics_bin[1]):<10.4f}{np.mean(metrics_bin[2]):<10.4f}{np.mean(metrics_bin[3]):<10.4f}
        median\t{np.median(metrics[0]):<10.4f}{np.median(metrics[1]):<10.4f}{np.median(metrics[2]):<10.4f}{np.median(metrics[3]):<10.4f}\t\t{np.median(metrics_bin[0]):<10.4f}{np.median(metrics_bin[1]):<10.4f}{np.median(metrics_bin[2]):<10.4f}{np.median(metrics_bin[3]):<10.4f}
        std\t{np.std(metrics[0]):<10.4f}{np.std(metrics[1]):<10.4f}{np.std(metrics[2]):<10.4f}{np.std(metrics[3]):<10.4f}\t\t{np.std(metrics_bin[0]):<10.4f}{np.std(metrics_bin[1]):<10.4f}{np.std(metrics_bin[2]):<10.4f}{np.std(metrics_bin[3]):<10.4f}
        ''')
    

    # TRAIN AND EVALUATE WITHOUT HOST_LTYPE
    metrics, _ = train_and_evaluate(merged_big_data.drop(columns=['host_ltype']), 'cpufailures', RandomForestClassifier(n_estimators=100, n_jobs=-1), binary=False)
    # binary classification
    metrics_bin, _ = train_and_evaluate(merged_big_data.drop(columns=['host_ltype']), 'cpufailures', RandomForestClassifier(n_estimators=100, n_jobs=-1), binary=True)

    print(f'''\t{'METRICS WITHOUT HOST_LTYPE':<48}\t{'METRICS WITHOUT HOST_LTYPE (binary)':<48}
        \t{'accuracy':<10}{'precision':<10}{'recall':<10}{'f1':<10}\t\t{'accuracy':<10}{'precision':<10}{'recall':<10}{'f1':<10}
        mean\t{np.mean(metrics[0]):<10.4f}{np.mean(metrics[1]):<10.4f}{np.mean(metrics[2]):<10.4f}{np.mean(metrics[3]):<10.4f}\t\t{np.mean(metrics_bin[0]):<10.4f}{np.mean(metrics_bin[1]):<10.4f}{np.mean(metrics_bin[2]):<10.4f}{np.mean(metrics_bin[3]):<10.4f}
        median\t{np.median(metrics[0]):<10.4f}{np.median(metrics[1]):<10.4f}{np.median(metrics[2]):<10.4f}{np.median(metrics[3]):<10.4f}\t\t{np.median(metrics_bin[0]):<10.4f}{np.median(metrics_bin[1]):<10.4f}{np.median(metrics_bin[2]):<10.4f}{np.median(metrics_bin[3]):<10.4f}
        std\t{np.std(metrics[0]):<10.4f}{np.std(metrics[1]):<10.4f}{np.std(metrics[2]):<10.4f}{np.std(metrics[3]):<10.4f}\t\t{np.std(metrics_bin[0]):<10.4f}{np.std(metrics_bin[1]):<10.4f}{np.std(metrics_bin[2]):<10.4f}{np.std(metrics_bin[3]):<10.4f}
        ''')
    

    # TRAIN AND EVALUATE WITHOUT HOST_LTYPE AND IPSCAP
    metrics, _ = train_and_evaluate(merged_big_data.drop(columns=['host_ltype', 'ipscap']), 'cpufailures', RandomForestClassifier(n_estimators=100, n_jobs=-1), binary=False)
    # binary classification
    metrics_bin, _ = train_and_evaluate(merged_big_data.drop(columns=['host_ltype', 'ipscap']), 'cpufailures', RandomForestClassifier(n_estimators=100, n_jobs=-1), binary=True)

    print(f'''\t{'METRICS WITHOUT HOST_LTYPE AND IPSCAP':<48}\t{'METRICS WITHOUT HOST_LTYPE AND IPSCAP (binary)':<48}
        \t{'accuracy':<10}{'precision':<10}{'recall':<10}{'f1':<10}\t\t{'accuracy':<10}{'precision':<10}{'recall':<10}{'f1':<10}
        mean\t{np.mean(metrics[0]):<10.4f}{np.mean(metrics[1]):<10.4f}{np.mean(metrics[2]):<10.4f}{np.mean(metrics[3]):<10.4f}\t\t{np.mean(metrics_bin[0]):<10.4f}{np.mean(metrics_bin[1]):<10.4f}{np.mean(metrics_bin[2]):<10.4f}{np.mean(metrics_bin[3]):<10.4f}
        median\t{np.median(metrics[0]):<10.4f}{np.median(metrics[1]):<10.4f}{np.median(metrics[2]):<10.4f}{np.median(metrics[3]):<10.4f}\t\t{np.median(metrics_bin[0]):<10.4f}{np.median(metrics_bin[1]):<10.4f}{np.median(metrics_bin[2]):<10.4f}{np.median(metrics_bin[3]):<10.4f}
        std\t{np.std(metrics[0]):<10.4f}{np.std(metrics[1]):<10.4f}{np.std(metrics[2]):<10.4f}{np.std(metrics[3]):<10.4f}\t\t{np.std(metrics_bin[0]):<10.4f}{np.std(metrics_bin[1]):<10.4f}{np.std(metrics_bin[2]):<10.4f}{np.std(metrics_bin[3]):<10.4f}
        ''')
    

    # TRAIN AND EVALUATE WITHOUT HOST_LTYPE, IPSCAP AND BASEIPS
    metrics, _ = train_and_evaluate(merged_big_data.drop(columns=['host_ltype', 'ipscap', 'baseips']), 'cpufailures', RandomForestClassifier(n_estimators=100, n_jobs=-1), binary=False)
    # binary classification
    metrics_bin, _ = train_and_evaluate(merged_big_data.drop(columns=['host_ltype', 'ipscap', 'baseips']), 'cpufailures', RandomForestClassifier(n_estimators=100, n_jobs=-1), binary=True)

    print(f'''\t{'METRICS WITHOUT HOST_LTYPE, IPSCAP AND BASEIPS':<48}\t{'METRICS WITHOUT HOST_LTYPE, IPSCAP AND BASEIPS (binary)':<48}
        \t{'accuracy':<10}{'precision':<10}{'recall':<10}{'f1':<10}\t\t{'accuracy':<10}{'precision':<10}{'recall':<10}{'f1':<10}
        mean\t{np.mean(metrics[0]):<10.4f}{np.mean(metrics[1]):<10.4f}{np.mean(metrics[2]):<10.4f}{np.mean(metrics[3]):<10.4f}\t\t{np.mean(metrics_bin[0]):<10.4f}{np.mean(metrics_bin[1]):<10.4f}{np.mean(metrics_bin[2]):<10.4f}{np.mean(metrics_bin[3]):<10.4f}
        median\t{np.median(metrics[0]):<10.4f}{np.median(metrics[1]):<10.4f}{np.median(metrics[2]):<10.4f}{np.median(metrics[3]):<10.4f}\t\t{np.median(metrics_bin[0]):<10.4f}{np.median(metrics_bin[1]):<10.4f}{np.median(metrics_bin[2]):<10.4f}{np.median(metrics_bin[3]):<10.4f}
        std\t{np.std(metrics[0]):<10.4f}{np.std(metrics[1]):<10.4f}{np.std(metrics[2]):<10.4f}{np.std(metrics[3]):<10.4f}\t\t{np.std(metrics_bin[0]):<10.4f}{np.std(metrics_bin[1]):<10.4f}{np.std(metrics_bin[2]):<10.4f}{np.std(metrics_bin[3]):<10.4f}
        ''')


    # TRAIN AND EVALUATE WITHOUT HOST_LTYPE, IPSCAP AND BASEIPS BUT WITH SVM
    metrics, _ = train_and_evaluate(merged_big_data.drop(columns=['host_ltype', 'ipscap', 'baseips']), 'cpufailures', SVC(), binary=False)
    # binary classification
    metrics_bin, _ = train_and_evaluate(merged_big_data.drop(columns=['host_ltype', 'ipscap', 'baseips']), 'cpufailures', SVC(), binary=True)

    print(f'''{'METRICS WITHOUT HOST_LTYPE, IPSCAP AND BASEIPS (SVM)':<56}\tMETRICS WITHOUT HOST_LTYPE, IPSCAP AND BASEIPS (SVM) (binary)
        \t{'accuracy':<10}{'precision':<10}{'recall':<10}{'f1':<10}\t\t{'accuracy':<10}{'precision':<10}{'recall':<10}{'f1':<10}
        mean\t{np.mean(metrics[0]):<10.4f}{np.mean(metrics[1]):<10.4f}{np.mean(metrics[2]):<10.4f}{np.mean(metrics[3]):<10.4f}\t\t{np.mean(metrics_bin[0]):<10.4f}{np.mean(metrics_bin[1]):<10.4f}{np.mean(metrics_bin[2]):<10.4f}{np.mean(metrics_bin[3]):<10.4f}
        median\t{np.median(metrics[0]):<10.4f}{np.median(metrics[1]):<10.4f}{np.median(metrics[2]):<10.4f}{np.median(metrics[3]):<10.4f}\t\t{np.median(metrics_bin[0]):<10.4f}{np.median(metrics_bin[1]):<10.4f}{np.median(metrics_bin[2]):<10.4f}{np.median(metrics_bin[3]):<10.4f}
        std\t{np.std(metrics[0]):<10.4f}{np.std(metrics[1]):<10.4f}{np.std(metrics[2]):<10.4f}{np.std(metrics[3]):<10.4f}\t\t{np.std(metrics_bin[0]):<10.4f}{np.std(metrics_bin[1]):<10.4f}{np.std(metrics_bin[2]):<10.4f}{np.std(metrics_bin[3]):<10.4f}
        ''')
    


    # TRAIN AND EVALUATE WITHOUT HOST_LTYPE, IPSCAP, BASEIPS AND IPSAVAILABLE
    metrics, _ = train_and_evaluate(merged_big_data.drop(columns=['host_ltype', 'ipscap', 'baseips', 'ipsavailable']), 'cpufailures', RandomForestClassifier(n_estimators=100, n_jobs=-1), binary=False)
    # binary classification
    metrics_bin, _ = train_and_evaluate(merged_big_data.drop(columns=['host_ltype', 'ipscap', 'baseips', 'ipsavailable']), 'cpufailures', RandomForestClassifier(n_estimators=100, n_jobs=-1), binary=True)

    print(f'''\t{'METRICS WITHOUT HOST_LTYPE, IPSCAP, BASEIPS AND IPSAVAILABLE':<48}\t{'METRICS WITHOUT HOST_LTYPE, IPSCAP, BASEIPS AND IPSAVAILABLE (binary)':<48}
        \t{'accuracy':<10}{'precision':<10}{'recall':<10}{'f1':<10}\t\t{'accuracy':<10}{'precision':<10}{'recall':<10}{'f1':<10}
        mean\t{np.mean(metrics[0]):<10.4f}{np.mean(metrics[1]):<10.4f}{np.mean(metrics[2]):<10.4f}{np.mean(metrics[3]):<10.4f}\t\t{np.mean(metrics_bin[0]):<10.4f}{np.mean(metrics_bin[1]):<10.4f}{np.mean(metrics_bin[2]):<10.4f}{np.mean(metrics_bin[3]):<10.4f}
        median\t{np.median(metrics[0]):<10.4f}{np.median(metrics[1]):<10.4f}{np.median(metrics[2]):<10.4f}{np.median(metrics[3]):<10.4f}\t\t{np.median(metrics_bin[0]):<10.4f}{np.median(metrics_bin[1]):<10.4f}{np.median(metrics_bin[2]):<10.4f}{np.median(metrics_bin[3]):<10.4f}
        std\t{np.std(metrics[0]):<10.4f}{np.std(metrics[1]):<10.4f}{np.std(metrics[2]):<10.4f}{np.std(metrics[3]):<10.4f}\t\t{np.std(metrics_bin[0]):<10.4f}{np.std(metrics_bin[1]):<10.4f}{np.std(metrics_bin[2]):<10.4f}{np.std(metrics_bin[3]):<10.4f}
        ''')
    

    # TRAIN AND EVALUATE WITHOUT HOST_LTYPE, IPSCAP, BASEIPS AND IPSAVAILABLE BUT WITH SVM
    metrics, _ = train_and_evaluate(merged_big_data.drop(columns=['host_ltype', 'ipscap', 'baseips', 'ipsavailable']), 'cpufailures', SVC(), binary=False)
    # binary classification
    metrics_bin, _ = train_and_evaluate(merged_big_data.drop(columns=['host_ltype', 'ipscap', 'baseips', 'ipsavailable']), 'cpufailures', SVC(), binary=True)

    print(f'''{'METRICS WITHOUT HOST_LTYPE, IPSCAP, BASEIPS AND IPSAVAILABLE (SVM)':<56}\tMETRICS WITHOUT HOST_LTYPE, IPSCAP, BASEIPS AND IPSAVAILABLE (SVM) (binary)
        \t{'accuracy':<10}{'precision':<10}{'recall':<10}{'f1':<10}\t\t{'accuracy':<10}{'precision':<10}{'recall':<10}{'f1':<10}
        mean\t{np.mean(metrics[0]):<10.4f}{np.mean(metrics[1]):<10.4f}{np.mean(metrics[2]):<10.4f}{np.mean(metrics[3]):<10.4f}\t\t{np.mean(metrics_bin[0]):<10.4f}{np.mean(metrics_bin[1]):<10.4f}{np.mean(metrics_bin[2]):<10.4f}{np.mean(metrics_bin[3]):<10.4f}
        median\t{np.median(metrics[0]):<10.4f}{np.median(metrics[1]):<10.4f}{np.median(metrics[2]):<10.4f}{np.median(metrics[3]):<10.4f}\t\t{np.median(metrics_bin[0]):<10.4f}{np.median(metrics_bin[1]):<10.4f}{np.median(metrics_bin[2]):<10.4f}{np.median(metrics_bin[3]):<10.4f}
        std\t{np.std(metrics[0]):<10.4f}{np.std(metrics[1]):<10.4f}{np.std(metrics[2]):<10.4f}{np.std(metrics[3]):<10.4f}\t\t{np.std(metrics_bin[0]):<10.4f}{np.std(metrics_bin[1]):<10.4f}{np.std(metrics_bin[2]):<10.4f}{np.std(metrics_bin[3]):<10.4f}
        ''')

    """

    # Train and Evaluate without host_ltype, ipscap, baseips and numcontainers
    # It was tested but the results were a lot worse

    # plot metrics
    # plot_metrics(metrics, 'big_merged_data_all_features')

    # Train and Evaluate with only the features that have a correlation with cpufailures
    # metrics, _ = train_and_evaluate(merged_big_data[['cpu','apparentips', 'cpufailures']], 'cpufailures', RandomForestClassifier(n_estimators=100, n_jobs=-1), binary=False)

    # plot metrics
    # plot_metrics(metrics, 'big_merged_data_correlated_features')

    # """

    # Remove ltype and ips cap?
    # normalize data?
    # remove outliers? is there any?
    
    
    # CPU SVM with grid search
    params = {
        'C': [0.1, 1, 10, 100, 1000],
        'gamma': [1, 0.1, 0.01, 0.001, 0.0001],
        'kernel': ['rbf', 'linear', 'poly', 'sigmoid']
    }
    
    # Train and Evaluate with all features
    print('CPU')
    print('all features')
    train_and_evaluate(merged_big_data_cpu, 'cpufailures', SVC(), binary=False, grid_search=True, param_grid=params)
    # binary classification
    print('binary classification')
    train_and_evaluate(merged_big_data_cpu, 'cpufailures', SVC(), binary=True, grid_search=True, param_grid=params)


    # RAM
    print('RAM')
    # Train and Evaluate with all features
    print('all features')
    train_and_evaluate(merged_big_data_ram, 'ramfailures', SVC(), binary=False, grid_search=True, param_grid=params)
    # binary classification
    print('binary classification')
    train_and_evaluate(merged_big_data_ram, 'ramfailures', SVC(), binary=True, grid_search=True, param_grid=params)
    
    exit()
    """
    # plot every feature against cpufailures
    for feature in merged_big_data.columns:
        if feature != 'cpufailures':
            # 2 subplots:
                # 1. scatter plot
                # 2. box plot

            plt.figure()
            fig, ax = plt.subplots(1,2, figsize=(10, 5), tight_layout=True)


            # 1. scatter plot
            sns.scatterplot(x='cpufailures', y=feature, data=merged_big_data, ax=ax[0])

            # 2. box plot
            sns.boxplot(x='cpufailures', y=feature, data=merged_big_data, ax=ax[1])

            plt.savefig(f'{big_analysis_path}pairs/{feature}_vs_numfailures.png')
            plt.savefig(f'{big_analysis_path}pairs/{feature}_vs_numfailures.svg')
    
    # Pairplot
    plt.figure()
    sns.pairplot(merged_big_data, hue='cpufailures')
    plt.savefig(f'{big_analysis_path}pairs/pairplot.png')
    plt.savefig(f'{big_analysis_path}pairs/pairplot.svg')

    """

    # select k best features
    # https://www.simplilearn.com/tutorials/machine-learning-tutorial/feature-selection-in-machine-learning
    # the aforementioned tutorial mentions that, for numerical input and categorical output, we should use ANOVA Correlation Coefficient (linear) or Kendall's rank coefficient (non-linear)

    print("\n---- SELECT K BEST FEATURES - RAM ----")

    print("\n\t-- ANOVA --")

    best_features = SelectKBest(score_func=f_classif, k="all")

    fit = best_features.fit(
        merged_big_data.drop(columns=["cpufailures", "ramfailures"]),
        merged_big_data["ramfailures"],
    )

    dfscores = pd.DataFrame(fit.scores_)
    dfcolumns = pd.DataFrame(
        merged_big_data.drop(columns=["cpufailures", "ramfailures"]).columns
    )

    featureScores = pd.concat([dfcolumns, dfscores], axis=1)
    featureScores.columns = ["Specs", "Score"]

    print(featureScores.sort_values(by="Score", ascending=False))

    print("\n\t-- CHI2 --")

    best_features = SelectKBest(score_func=chi2, k="all")

    fit = best_features.fit(
        merged_big_data.drop(columns=["cpufailures", "ramfailures"]),
        merged_big_data["ramfailures"],
    )

    dfscores = pd.DataFrame(fit.scores_)
    dfcolumns = pd.DataFrame(
        merged_big_data.drop(columns=["cpufailures", "ramfailures"]).columns
    )

    featureScores = pd.concat([dfcolumns, dfscores], axis=1)
    featureScores.columns = ["Specs", "Score"]

    print(featureScores.sort_values(by="Score", ascending=False))

    # feature importance
    print("\n---- FEATURE IMPORTANCE ----")

    model = ExtraTreesClassifier()
    model.fit(
        merged_big_data.drop(columns=["cpufailures", "ramfailures"]),
        merged_big_data["ramfailures"],
    )

    print(model.feature_importances_)

    feat_importances = pd.Series(
        model.feature_importances_,
        index=merged_big_data.drop(columns=["cpufailures", "ramfailures"]).columns,
    )
    print(feat_importances.sort_values(ascending=False))

    print("\n---- SELECT K BEST FEATURES - CPU ----")

    print("\n\t-- ANOVA --")

    best_features = SelectKBest(score_func=f_classif, k="all")

    fit = best_features.fit(
        merged_big_data.drop(columns=["cpufailures", "ramfailures"]),
        merged_big_data["ramfailures"],
    )

    dfscores = pd.DataFrame(fit.scores_)
    dfcolumns = pd.DataFrame(
        merged_big_data.drop(columns=["cpufailures", "ramfailures"]).columns
    )

    featureScores = pd.concat([dfcolumns, dfscores], axis=1)
    featureScores.columns = ["Specs", "Score"]

    print(featureScores.sort_values(by="Score", ascending=False))

    """
    print('\n\t-- KENDALL --')

    best_features = SelectKBest(score_func=kendalltau, k='all')

    fit = best_features.fit(merged_big_data.drop(columns=['cpufailures', 'ramfailures']), merged_big_data['ramfailures'])

    dfscores = pd.DataFrame(fit.scores_)
    dfcolumns = pd.DataFrame(merged_big_data.drop(columns=['cpufailures', 'ramfailures']).columns)

    featureScores = pd.concat([dfcolumns, dfscores], axis=1)
    featureScores.columns = ['Specs', 'Score']

    print(featureScores.sort_values(by='Score', ascending=False))
    """

    print("\n\t-- CHI2 --")

    best_features = SelectKBest(score_func=chi2, k="all")

    fit = best_features.fit(
        merged_big_data.drop(columns=["cpufailures", "ramfailures"]),
        merged_big_data["ramfailures"],
    )

    dfscores = pd.DataFrame(fit.scores_)
    dfcolumns = pd.DataFrame(
        merged_big_data.drop(columns=["cpufailures", "ramfailures"]).columns
    )

    featureScores = pd.concat([dfcolumns, dfscores], axis=1)
    featureScores.columns = ["Specs", "Score"]

    print(featureScores.sort_values(by="Score", ascending=False))

    # feature importance
    print("\n---- FEATURE IMPORTANCE ----")

    model = ExtraTreesClassifier()
    model.fit(
        merged_big_data.drop(columns=["cpufailures", "ramfailures"]),
        merged_big_data["ramfailures"],
    )

    print(model.feature_importances_)

    feat_importances = pd.Series(
        model.feature_importances_,
        index=merged_big_data.drop(columns=["cpufailures", "ramfailures"]).columns,
    )
    print(feat_importances.sort_values(ascending=False))


def test():
    # plot ram from datasets

    # read data 1
    datapath = (
        "logs/MyFog_MyAzure2019Workload_100_6_30_10000_300_1/hostinfo_with_interval.csv"
    )
    data = pd.read_csv(datapath)

    num_hosts = 3

    headers = [
        "ram_s",
        "ram_r",
        "ram_w",
        "ramavailable_s",
        "ramavailable_r",
        "ramavailable_w",
    ]

    data = data[["interval"] + headers]

    hosts = [data.copy() for _ in range(num_hosts)]

    for i in range(num_hosts):
        for h in headers:
            hosts[i][h] = hosts[i][h].apply(lambda x: ast.literal_eval(x)[i])

        print(hosts[i].head())
        print(hosts[i].describe())

    # plot with 'interval' as x axis

    # polot horizontal line in each plot
    list_ram = [[4295, 17180, 34360], [372.0, 360.0, 376.54], [200.0, 305.0, 266.75]]

    _, ax = plt.subplots(3, 3, figsize=(15, 10))
    for i, host in enumerate(hosts):
        for r in ["ram", "ramavailable"]:
            for j, hh in enumerate(["s", "r", "w"]):
                ax[i][j].axhline(y=list_ram[j][i], color="r", linestyle="-")
                sns.lineplot(x="interval", y=f"{r}_{hh}", data=host, ax=ax[i][j])

    plt.savefig("ram.png")
    plt.savefig("ram.svg")


def train_and_evaluate(data, y_col, model, data_test=None, binary=False, grid_search=False, param_grid=None):
    """
    Train and evaluate a model using the given data and model
    It will run NUMBER_OF_REPETITIONS times

    Parameters
    ----------
    data : pandas.DataFrame
        Data to train and evaluate the model
    y_col : str
        Name of the column to predict
    model : sklearn.model
        Model to train and evaluate
    data_test : pandas.DataFrame, optional
        Test data to evaluate the model, by default None
    binary : bool, optional
        If True, the data will be converted to binary, by default False

    Returns
    -------
    list
        List of metrics [accuracy, precision, recall, f1]
    tuple
        Tuple with the best predicted values and respective f1 score
    """

    if binary:
        data[y_col] = data[y_col].apply(lambda x: 1 if x > 0 else 0)

    if grid_search:
        if not param_grid:
            print("param_grid must be provided for grid search")
            return
        
        # split data if test data is not provided
        if data_test is None:
            train, test = train_test_split(data, test_size=0.3, shuffle=True)
        else:
            train = data
            test = data_test

        x_train = train.drop(columns=[y_col])
        y_train = train[y_col]

        x_test = test.drop(columns=[y_col])
        y_test = test[y_col]

        grid = GridSearchCV(model, param_grid, refit=True, verbose=3, n_jobs=-1)

        grid.fit(x_train, y_train)

        print(grid.best_params_)
        print(grid.best_estimator_)

        y_pred = grid.predict(x_test)

        print(classification_report(y_test, y_pred))
        
        return

        

    metrics = [[], [], [], []]
    best_f1 = 0
    y_pred_best = None
    for _ in range(NUMBER_OF_REPETITIONS):
        # split data if test data is not provided
        if data_test is None:
            train, test = train_test_split(data, test_size=0.3, shuffle=True)
        else:
            train = data
            test = data_test

        x_train = train.drop(columns=[y_col])
        y_train = train[y_col]

        x_test = test.drop(columns=[y_col])
        y_test = test[y_col]

        # train and predict
        model.fit(x_train, y_train)
        y_pred = model.predict(x_test)

        # evaluate
        metrics[0].append(accuracy_score(y_test, y_pred))
        metrics[1].append(
            precision_score(y_test, y_pred, average="binary" if binary else "macro")
        )
        metrics[2].append(
            recall_score(y_test, y_pred, average="binary" if binary else "macro")
        )
        metrics[3].append(
            f1_score(y_test, y_pred, average="binary" if binary else "macro")
        )

        # Why macro? https://towardsdatascience.com/micro-macro-weighted-averages-of-f1-score-clearly-explained-b603420b292f
        # Because the classes are imbalanced, so we want to give the same importance to each class. average='weighted' would give more importance to the majority class

        # copilot sugested the following link: https://datascience.stackexchange.com/questions/15989/micro-average-vs-macro-average-performance-in-a-multiclass-classification-settin

        if metrics[3][-1] > best_f1:
            best_f1 = metrics[3][-1]
            y_pred_best = y_pred

    return metrics, (y_pred_best, best_f1)


def plot_metrics(metrics, name):
    """
    Plot the metrics

    Parameters
    ----------
    metrics : list
        List of metrics to plot [accuracy, precision, recall, f1]
    name : str
        Name of the file to save the plot
    """

    plt.figure()
    plt.boxplot(metrics)
    plt.xticks([1, 2, 3, 4], ["Accuracy", "Precision", "Recall", "F1"])
    plt.savefig(f"{FIGURES_PATH}metrics/{name}.png")
    plt.savefig(f"{FIGURES_PATH}metrics/{name}.svg")


def multiprocessing_test():
    """
    for i in range(NUMBER_OF_SIMULATIONS):
        print(f'starting -> {i}')
        # do some hard work
        a = 0
        for j in range(NUM_SIM_STEPS*10):
            # do some calculations
            for _ in range(j, NUM_SIM_STEPS*10):
                a += 1
        print(f'finished -> {i}, result: {a}')
    """

    # same as above but using multiprocessing
    with Pool(processes=NUMBER_OF_SIMULATIONS) as pool:
        pool.map(multiprocessing_test_aux, range(NUMBER_OF_SIMULATIONS))


def multiprocessing_test_aux(i):
    print(f"starting -> {i}")
    # do some hard work
    a = 0
    for j in range(NUM_SIM_STEPS * 10):
        # do some calculations
        for _ in range(j, NUM_SIM_STEPS * 10):
            a += 1
    print(f"finished -> {i}, result: {a}")


if __name__ == "__main__":
    time_start = time.time()

    # generate_datasets()

    # failure_distribution()

    # plot_data()

    # train_and_evaluate_big_data()

    big_merged_data_eda()

    # test()

    # multiprocessing_test()

    print(f"Time taken: {time.time() - time_start}")
