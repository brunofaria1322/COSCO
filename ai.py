import ast
import time
import pandas as pd
import numpy as np
import json
import os
import matplotlib.pyplot as plt

import seaborn as sns

# sns color palette
COLOR_PALETTE = "hls"
sns.set_palette(COLOR_PALETTE)


from matplotlib.colors import ListedColormap


from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier
from sklearn.feature_selection import SelectKBest, chi2, f_classif
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import (
    accuracy_score,
    classification_report,
    confusion_matrix,
    f1_score,
    make_scorer,
    precision_score,
    recall_score,
)

from cosco import (
    runCOSCO,
    NUM_SIM_STEPS,
    HOSTS,
    CONTAINERS,
    ROUTER_BW,
    INTERVAL_TIME,
    NEW_CONTAINERS,
    FAULT_RATE,
    FAULT_TIME,
    FAULT_INCREASE_TIME,
    RECOVER_TIME,
    FAULTY_HOSTS,
    FAILURE_TYPES,
    ACCUMULATIVE_FAULTS,
)

# from cosco import runCOSCO, NUM_SIM_STEPS, FAULT_INCREASE_TIME, FAULTY_HOSTS, ACCUMULATIVE_FAULTS

# FAULT_RATE = 0.3
# FAULT_TIME = 6
# RECOVER_TIME = 18


hosts_str = "".join([str(i) for i in FAULTY_HOSTS])
type_str = "acc" if ACCUMULATIVE_FAULTS else "rec"
fault_type_str = "".join([str(i[0]).lower() for i in FAILURE_TYPES])

DATAPATH = f"AI/backups/{NUM_SIM_STEPS}i_{FAULT_RATE}fr_{FAULT_TIME}ft_{RECOVER_TIME}rt_{FAULT_INCREASE_TIME}fit_hosts{hosts_str}_{type_str}_{fault_type_str}/"
FIGURES_PATH = f"{DATAPATH}/figures/"
CSV_PATH = f"logs/MyFog_MyAzure2019Workload_{NUM_SIM_STEPS}_{HOSTS}_{CONTAINERS}_{ROUTER_BW}_{INTERVAL_TIME}_{NEW_CONTAINERS}/hostinfo_with_interval.csv"

NUMBER_OF_SIMULATIONS = 30

NUMBER_OF_REPETITIONS = 50


def generate_datasets():
    """
    Generates datasets for the AI by calling the COSCO simulator
    Will generate NUMBER_OF_SIMULATIONS datasets

    """
    # create datapath folder if it doesn't exist

    os.makedirs(os.path.dirname(DATAPATH), exist_ok=True)
    os.makedirs(os.path.dirname(DATAPATH + "data/"), exist_ok=True)
    os.makedirs(os.path.dirname(FIGURES_PATH), exist_ok=True)
    os.makedirs(os.path.dirname(FIGURES_PATH + "analysis/"), exist_ok=True)
    os.makedirs(os.path.dirname(FIGURES_PATH + "metrics/"), exist_ok=True)

    # old version (without multiprocessing)
    for i in range(NUMBER_OF_SIMULATIONS):
        datapath_i = f"{DATAPATH}data/data{i}.csv"
        # skip if log file already exists
        if os.path.isfile(datapath_i):
            continue

        print(f"Creating DATA {i+1} of {NUMBER_OF_SIMULATIONS}")
        # run simulation
        runCOSCO(prints=False)

        # copy log file to datapath
        os.system(f"cp {CSV_PATH} {datapath_i}")


def call_cosco(i):
    """
    Calls the COSCO simulator and saves the log file to the datapath (dataset)

    Parameters
    ----------
    i : int
        The index of the dataset to be generated
    """

    datapath_i = f"{DATAPATH}data/data{i}.csv"
    # skip if log file already exists
    if os.path.isfile(datapath_i):
        return

    print(f"Creating DATA {i+1} of {NUMBER_OF_SIMULATIONS}")
    # run simulation
    runCOSCO(prints=False)

    # copy log file to datapath
    # check if file exists
    if os.path.isfile(CSV_PATH):
        os.system(f"cp {CSV_PATH} {datapath_i}")


def evaluate_datasets(failure="cpu"):
    """
    Evaluates the datasets generated by the COSCO simulator



    """
    # EVALUATING DATA
    metrics_1 = [[], [], [], []]  # accuracy, precision, recall, f1
    metrics_all = [[], [], [], []]  # accuracy, precision, recall, f1
    metrics_12_3 = [[], [], [], []]  # accuracy, precision, recall, f1

    best_f1 = 0
    best_pred = None
    best_cpu = None

    metrics_path = FIGURES_PATH + "metrics/"

    for i in range(NUMBER_OF_SIMULATIONS):
        print(f"Evaluating DATA {i+1} of {NUMBER_OF_SIMULATIONS}")

        datapath_i = f"{DATAPATH}data/data{i}.csv"

        # read data
        data = pd.read_csv(datapath_i)

        # data has lists on each column. in this case, we want only the first element of each list
        # remove some columns that are not needed for now

        headers = [
            "interval",
            "cpu",
            "numcontainers",
            "baseips",
            "ipsavailable",
            "ipscap",
            "apparentips",
        ]

        data = data[headers + "cpufailures"]

        # create copies of data
        host1 = data.copy()
        host2 = data.copy()
        host3 = data.copy()

        for header in headers:
            host1[header] = host1[header].apply(lambda x: json.loads(x)[0])
            host2[header] = host2[header].apply(lambda x: json.loads(x)[1])
            host3[header] = host3[header].apply(lambda x: json.loads(x)[2])

        # count failures
        # print("Class distribution:")
        # print("Host1:\n", host1['cpufailures'].value_counts())
        # print("Host2:\n", host2['cpufailures'].value_counts())
        # print("Host3:\n", host3['cpufailures'].value_counts())

        # WORK WITH BINARY CLASSIFICATION
        host1["cpufailures"] = host1["cpufailures"].apply(lambda x: 1 if x > 0 else 0)
        host2["cpufailures"] = host2["cpufailures"].apply(lambda x: 1 if x > 0 else 0)
        host3["cpufailures"] = host3["cpufailures"].apply(lambda x: 1 if x > 0 else 0)

        # print("Class distribution after binary classification:")
        # print("Host1:\n", host1['cpufailures'].value_counts())
        # print("Host2:\n", host2['cpufailures'].value_counts())
        # print("Host3:\n", host3['cpufailures'].value_counts())

        # TRAIN AND EVALUATE ONLY ON HOST1
        metrics_temp, _ = train_and_evaluate(
            host1,
            "cpufailures",
            RandomForestClassifier(n_estimators=100, n_jobs=-1),
            binary=True,
        )
        metrics_1[0].extend(metrics_temp[0])
        metrics_1[1].extend(metrics_temp[1])
        metrics_1[2].extend(metrics_temp[2])
        metrics_1[3].extend(metrics_temp[3])

        # TRAIN AND EVALUATE ON ALL HOSTS TOGETHER

        # concatenate data
        all_hosts = pd.concat([host1, host2, host3])

        metrics_temp, _ = train_and_evaluate(
            all_hosts,
            "cpufailures",
            RandomForestClassifier(n_estimators=100, n_jobs=-1),
            binary=True,
        )
        metrics_all[0].extend(metrics_temp[0])
        metrics_all[1].extend(metrics_temp[1])
        metrics_all[2].extend(metrics_temp[2])
        metrics_all[3].extend(metrics_temp[3])
        # TRAIN ON HOSTS 1 AND 2, EVALUATE ON HOST 3

        # concatenate data
        host1_2 = pd.concat([host1, host2])

        metrics_temp, best_info = train_and_evaluate(
            host1_2,
            "cpufailures",
            RandomForestClassifier(n_estimators=100, n_jobs=-1),
            data_test=host3,
            binary=True,
        )
        metrics_12_3[0].extend(metrics_temp[0])
        metrics_12_3[1].extend(metrics_temp[1])
        metrics_12_3[2].extend(metrics_temp[2])
        metrics_12_3[3].extend(metrics_temp[3])

        if best_info[1] > best_f1:
            best_f1 = best_info[1]
            best_pred = best_info[0]
            best_cpu = host3["cpu"].values

    # plot histograms for f1 scores
    plt.figure()
    plt.hist(metrics_1[3], bins=10, alpha=0.5)
    plt.hist(metrics_all[3], bins=10, alpha=0.5)
    plt.hist(metrics_12_3[3], bins=10, alpha=0.5)
    plt.legend(["Host1", "All hosts", "Host1 and Host2"])
    plt.savefig(f"{metrics_path}/f1_scores.png")
    plt.savefig(f"{metrics_path}/f1_scores.svg")

    # plots for host1
    plot_metrics(metrics_1, "metrics_1")

    # plots for all hosts
    plot_metrics(metrics_all, "metrics_all")

    # plots for train on host1 and host2, test on host3
    plot_metrics(metrics_12_3, "metrics_12_3")

    # plot cpu usage with the color of the confusion label
    classes = []
    for i in range(len(best_pred)):
        if best_pred[i] == best_cpu.iloc[:, -1].values[i]:
            if best_pred[i] == 1:
                # True Positive
                classes.append(1)
            else:
                # True Negative
                classes.append(0)
        else:
            if best_pred[i] == 1:
                # False Positive
                classes.append(3)
            else:
                # False Negative
                classes.append(2)

    colors = ListedColormap(["blue", "green", "yellow", "orange"])

    plt.figure(figsize=(15, 5))
    scatter = plt.scatter(range(len(best_pred)), best_cpu, c=classes, cmap=colors)
    plt.xlabel("Time")
    plt.ylabel("CPU usage")
    plt.legend(
        handles=scatter.legend_elements()[0],
        loc="upper left",
        labels=["True Negative", "True Positive", "False Negative", "False Positive"],
    )

    plt.savefig(f"{metrics_path}/cpu_12_3.png")
    plt.savefig(f"{metrics_path}/cpu_12_3.svg")


def train_and_evaluate_big_data():
    data_temp = pd.read_csv(f"{DATAPATH}data/data0.csv")

    num_hosts = int(len(json.loads(data_temp["cpu"][0])) / 2)

    # create big data dataframe
    big_data = [pd.DataFrame() for _ in range(num_hosts)]

    for i in range(NUMBER_OF_SIMULATIONS):
        datapath_i = f"{DATAPATH}data/data{i}.csv"
        data_temp = pd.read_csv(datapath_i)
        # print(f'Number of hosts: {num_hosts}')

        data_temp = data_temp.drop(
            columns=["interval", "ram", "ramavailable", "disk", "diskavailable"]
        )
        # get headers
        headers = data_temp.columns

        # create list of copies of data
        data = [data_temp.copy() for _ in range(num_hosts)]

        for j in range(num_hosts):
            for header in headers:
                data[j][header] = data[j][header].apply(lambda x: json.loads(x)[j])

            # append data to big data
            big_data[j] = big_data[j].append(data[j])

    for i in range(num_hosts):
        big_data[i] = big_data[i].reset_index(drop=True)

    # train and evaluate
    metrics = [[], [], [], []]
    for _ in range(NUMBER_OF_REPETITIONS):
        # split data
        train, test = train_test_split(big_data[0], test_size=0.2)

        # train model
        clf = RandomForestClassifier(n_estimators=100)
        clf.fit(train.iloc[:, :-1], train.iloc[:, -1])

        # predict
        pred = clf.predict(test.iloc[:, :-1])

        # evaluate
        metrics[0].append(accuracy_score(test.iloc[:, -1], pred))
        metrics[1].append(precision_score(test.iloc[:, -1], pred, average="macro"))
        metrics[2].append(recall_score(test.iloc[:, -1], pred, average="macro"))
        metrics[3].append(
            0
            if metrics[1][-1] * metrics[2][-1] == 0
            else 2
            * (metrics[1][-1] * metrics[2][-1])
            / (metrics[1][-1] + metrics[2][-1])
        )

    # plot metrics
    plt.figure()
    plt.boxplot(metrics)
    plt.xticks([1, 2, 3, 4], ["Accuracy", "Precision", "Recall", "F1"])
    plt.savefig(f"{FIGURES_PATH}/metrics_big_data.png")
    plt.savefig(f"{FIGURES_PATH}/metrics_big_data.svg")

    # train in all data
    merged_big_data = pd.DataFrame()
    for i in range(num_hosts):
        merged_big_data = merged_big_data.append(big_data[i])

    merged_big_data = merged_big_data.reset_index(drop=True)
    print(merged_big_data.shape)

    # split data
    train, test = train_test_split(merged_big_data, test_size=0.2)

    # train model
    clf = RandomForestClassifier(n_estimators=100)
    clf.fit(train.iloc[:, :-1], train.iloc[:, -1])

    # predict
    pred = clf.predict(test.iloc[:, :-1])

    # evaluate
    metrics = []
    metrics.append(accuracy_score(test.iloc[:, -1], pred))
    metrics.append(precision_score(test.iloc[:, -1], pred, average="macro"))
    metrics.append(recall_score(test.iloc[:, -1], pred, average="macro"))
    metrics.append(
        0
        if metrics[1] * metrics[2] == 0
        else 2 * (metrics[1] * metrics[2]) / (metrics[1] + metrics[2])
    )

    print(metrics)


def plot_distribution(data, dataset_index):
    num_hosts = len(data)

    # limit color palette to number of hosts
    colors = sns.color_palette(COLOR_PALETTE, num_hosts)

    # count number of cpu failures
    counts_cpu = [list(host["cpufailures"].value_counts()) for host in data]

    # count number of ram failures
    counts_ram = [list(host["ramfailures"].value_counts()) for host in data]

    num_max_labels = max(
        [
            max([len(count)] for count in counts_cpu)[0],
            max([len(count) for count in counts_ram]),
        ]
    )
    # print(f'Number of labels: {num_max_labels}')

    for count in counts_cpu:
        while len(count) < num_max_labels:
            count.append(0)

    for count in counts_ram:
        while len(count) < num_max_labels:
            count.append(0)

    plt.figure()
    fig, ax = plt.subplots(figsize=(10, 5), tight_layout=True)

    x = np.arange(num_max_labels)
    x_labels = [str(label) for label in range(num_max_labels)]

    width = 1 / ((num_hosts * 2) + 1)
    multiplier = 0

    for h_i in range(num_hosts):
        offset = width * multiplier
        multiplier += 1

        # cpu failures
        rects = ax.bar(
            x + offset,
            counts_cpu[h_i],
            width,
            label=f"Host {h_i}",
            color=colors[h_i],
        )

        for rect in rects:
            height = rect.get_height()

            if height > 0:
                ax.annotate(
                    f"{height}",
                    xy=(rect.get_x() + rect.get_width() / 2, height),
                    xytext=(0, 3),
                    textcoords="offset points",
                    ha="center",
                    va="bottom",
                )

        offset = width * (multiplier + num_hosts - 1)

        # ram failures
        rects = ax.bar(
            x + offset, counts_ram[h_i], width, hatch="///", color=colors[h_i]
        )

        for rect in rects:
            height = rect.get_height()

            if height > 0:
                ax.annotate(
                    f"{height}",
                    xy=(rect.get_x() + rect.get_width() / 2, height),
                    xytext=(0, 3),
                    textcoords="offset points",
                    ha="center",
                    va="bottom",
                )

    ax.set_xlabel("Stress Intensity")
    ax.set_ylabel("Number of Occurrences")

    ax.set_xticks(x + (num_hosts * 2 - 1) / 2 * width)
    ax.set_xticklabels(x_labels)

    # add invisible data to add second legend
    ax.bar(1, 0, color="gray", label="CPU")
    ax.bar(1, 0, color="gray", hatch="///", label="RAM")

    ax.legend(loc="upper right")

    plt.savefig(
        f"{FIGURES_PATH}analysis/individuals/data{dataset_index}/png/failure_distribution.png"
    )
    plt.savefig(
        f"{FIGURES_PATH}analysis/individuals/data{dataset_index}/svg/failure_distribution.svg"
    )


def plot_cpu_ram(data, dataset_index):
    num_hosts = int(len(data)/2)
    individual_data_path = f"{FIGURES_PATH}analysis/individuals/data{dataset_index}/"

    def plot_usage_and_failures(component):
        component_failures = f"{component}failures"
        
        fig_hos, ax_hos = plt.subplots(
            nrows=num_hosts, ncols=1, sharex=True, sharey=True, figsize=(10, 10)
        )

        fig_all, ax_all = plt.subplots(
            nrows=num_hosts * 2, ncols=1, sharex=True, sharey=True, figsize=(10, 15)
        )

        # each row represents a different host
        # x = ["interval"]
        # y = [component]
        # color intervals according to failure intensity [component_failures]

        most_failures = 0
        most_failures_index = 0

        scs_hos = [None for _ in range(num_hosts)]

        for h_i in range(num_hosts * 2):
            # print(f'Host {h_i}')
            # print(f'{component.upper()}: {data[h_i][component]}')
            # print(f'{component.upper()} Failures: {data[h_i][component_failures]}')

            ### INDIVIDUAL ###
            fig_ind, ax_ind = plt.subplots(figsize=(10, 5))
            ax_ind.plot(
                data[h_i]["interval"],
                data[h_i][component],
                color="black",
                label=f"{component.upper()} Usage",
            )

            if h_i < num_hosts:
                # component failures
                sc = ax_ind.scatter(
                    data[h_i]["interval"],
                    data[h_i][component],
                    c=data[h_i][component_failures],
                    cmap="magma_r",
                )

                fig_ind.legend(
                    *sc.legend_elements(),
                    bbox_to_anchor=(1.13, 0.62),
                    title="Stress Intensity",
                )

            ax_ind.set_xlabel("Interval")
            ax_ind.set_ylabel(f"{component.upper()} Usage (%)")

            ax_ind.set_xlim([0, len(data[h_i]["interval"])])
            # ax_ind.set_ylim([0, 100])

            fig_ind.legend(*ax_ind.get_legend_handles_labels(), bbox_to_anchor=(1.13, 0.72))

            fig_ind.tight_layout()
            fig_ind.savefig(f"{individual_data_path}png/indiv/{component}_{h_i}.png")
            fig_ind.savefig(f"{individual_data_path}svg/indiv/{component}_{h_i}.svg")

            ### HOST PLOTS ###
            if h_i < num_hosts:
                if max(data[h_i][component_failures]) > most_failures:
                    most_failures = max(data[h_i][component_failures])
                    most_failures_index = h_i

                # component usage
                ax_hos[h_i].plot(
                    data[h_i]["interval"],
                    data[h_i][component],
                    color="black",
                    label=f"{component.upper()} Usage",
                )

                # component failures
                scs_hos[h_i] = ax_hos[h_i].scatter(
                    data[h_i]["interval"],
                    data[h_i][component],
                    c=data[h_i][component_failures],
                    cmap="magma_r",
                )

                #ax_hos[h_i].set_ylim([0, 100])
                ax_hos[h_i].set_xlim([0, len(data[h_i]["interval"])])
                ax_hos[h_i].set_title(f"Host {h_i}")

            ### ALL PLOTS (with replicas) ###
            # component usage
            ax_all[h_i].plot(
                data[h_i]["interval"],
                data[h_i][component],
                color="black",
                label=f"{component.upper()} Usage",
            )

            if h_i < num_hosts:

                # component failures
                ax_all[h_i].scatter(
                    data[h_i]["interval"],
                    data[h_i][component],
                    c=data[h_i][component_failures],
                    cmap="magma_r",
                )

            #ax_all[h_i].set_ylim([0, 100])
            ax_all[h_i].set_xlim([0, len(data[h_i]["interval"])])

        # HOST PLOTS
        # xlabel and ylabel in the middle
        fig_hos.supylabel(f"{component.upper()} Usage (%)")
        fig_hos.supxlabel("Interval")


        fig_hos.legend(*ax_hos[0].get_legend_handles_labels(), bbox_to_anchor=(1.13, 0.72))
        fig_hos.legend(
            *scs_hos[most_failures_index].legend_elements(),
            bbox_to_anchor=(1.13, 0.62),
            title="Stress Intensity",
        )

        fig_hos.tight_layout()
        fig_hos.savefig(f"{individual_data_path}png/{component}.png")
        fig_hos.savefig(f"{individual_data_path}png/{component}.svg")

        # ALL PLOTS
        # xlabel and ylabel in the middle
        fig_all.supylabel(f"{component.upper()} Usage (%)")
        fig_all.supxlabel("Interval")


        fig_all.legend(*ax_all[0].get_legend_handles_labels(), bbox_to_anchor=(1.13, 0.72))
        fig_all.legend(
            *scs_hos[most_failures_index].legend_elements(),
            bbox_to_anchor=(1.13, 0.62),
            title="Stress Intensity",
        )

        fig_all.tight_layout()
        fig_all.savefig(f"{individual_data_path}png/{component}_all.png")
        fig_all.savefig(f"{individual_data_path}png/{component}_all.svg")

    plot_usage_and_failures("cpu")
    plot_usage_and_failures("ram")


def plot_data():
    os.makedirs(os.path.dirname(FIGURES_PATH + "analysis/"), exist_ok=True)
    individual_path = FIGURES_PATH + "analysis/individuals/"
    os.makedirs(os.path.dirname(individual_path), exist_ok=True)

    for i in range(NUMBER_OF_SIMULATIONS):
        os.makedirs(os.path.dirname(f"{individual_path}data{i}/"), exist_ok=True)
        os.makedirs(os.path.dirname(f"{individual_path}data{i}/png/"), exist_ok=True)
        os.makedirs(os.path.dirname(f"{individual_path}data{i}/png/indiv/"), exist_ok=True)
        os.makedirs(os.path.dirname(f"{individual_path}data{i}/svg/"), exist_ok=True)
        os.makedirs(os.path.dirname(f"{individual_path}data{i}/svg/indiv/"), exist_ok=True)

        datapath_i = f"{DATAPATH}data/data{i}.csv"
        data_temp = pd.read_csv(datapath_i)

        num_hosts = int(len(json.loads(data_temp["cpu"][0])) / 2)
        # print(f'Number of hosts: {num_hosts}')

        data_temp = data_temp.drop(columns=["disk", "diskavailable"])
        # get headers
        headers = data_temp.columns

        # create list of copies of data
        data = [data_temp.copy() for _ in range(2 * num_hosts)]

        for j in range(len(data)):
            for header in headers:
                if header != "interval":
                    data[j][header] = data[j][header].apply(lambda x: json.loads(x)[j])

        plot_distribution(
            data[:num_hosts],   # only hosts, not replicas
            i,
        )

        plot_cpu_ram(data, i)

        # plot number pf containers
        fig, ax = plt.subplots(
            nrows=num_hosts, ncols=1, sharex=True, sharey=True, figsize=(10, 5)
        )

        for h_i in range(num_hosts):
            # INDIVIDUAL PLOT
            fig_in, ax_in = plt.subplots(figsize=(10, 5))
            ax_in.plot(
                data[h_i]["interval"],
                data[h_i]["numcontainers"]
            )

            # SUBPLOT

            # component usage
            ax[h_i].plot(
                data[h_i]["interval"],
                data[h_i]["numcontainers"]
            )

            #ax[h_i].set_ylim([0, 100])
            ax[h_i].set_xlim([0, len(data[h_i]["interval"])])
            ax[h_i].set_title(f"Host {h_i}")

        # ylabel in the middle
        ax[np.floor(num_hosts / 2).astype(int)].set_ylabel(
            f"Number of Containers", loc="center"
        )
        ax[-1].set_xlabel("Interval")
        plt.tight_layout()
        plt.savefig(f"{individual_path}data{i}/numcontainers.png")
        plt.savefig(f"{individual_path}data{i}/numcontainers.svg")


def big_merged_data_eda():
    # Exploratory Data Analysis on the merged data

    big_analysis_path = FIGURES_PATH + "analysis/big_merged_data_eda/"
    os.makedirs(os.path.dirname(big_analysis_path), exist_ok=True)

    os.makedirs(os.path.dirname(big_analysis_path + "pairs/"), exist_ok=True)
    os.makedirs(os.path.dirname(big_analysis_path + "pairs/cpu/"), exist_ok=True)
    os.makedirs(os.path.dirname(big_analysis_path + "pairs/ram/"), exist_ok=True)

    os.makedirs(os.path.dirname(big_analysis_path + "dim_red/"), exist_ok=True)

    # load and merge data

    cpu_headers = {
        "cpu": float,
        "numcontainers": int,
        "baseips": float,
        "ipsavailable": float,
        "ipscap": float,
        "apparentips": float,
        "cpufailures": int,
    }

    ram_headers = {
        "ram": float,
        "numcontainers": int,
        "ram_s": float,
        "ram_r": float,
        "ram_w": float,
        "ramavailable_s": float,
        "ramavailable_r": float,
        "ramavailable_w": float,
        "ramfailures": int,
    }

    # create list for all dataframes
    data_cpu = [None] * NUMBER_OF_SIMULATIONS
    data_ram = [None] * NUMBER_OF_SIMULATIONS

    for i in range(NUMBER_OF_SIMULATIONS):
        datapath_i = f"{DATAPATH}data/data{i}.csv"
        data_temp = pd.read_csv(datapath_i)

        num_hosts = int(len(json.loads(data_temp["cpu"][0])) / 2)

        data_temp_cpu = data_temp[cpu_headers.keys()]
        data_temp_ram = data_temp[ram_headers.keys()]

        data_temp_cpu = data_temp_cpu.applymap(
            lambda x: json.loads(x)[:num_hosts]
        ).apply(pd.Series.explode)
        data_temp_ram = data_temp_ram.applymap(
            lambda x: json.loads(x)[:num_hosts]
        ).apply(pd.Series.explode)

        data_cpu[i] = data_temp_cpu
        data_ram[i] = data_temp_ram

    merged_big_data_cpu = pd.concat(data_cpu, ignore_index=True)
    merged_big_data_ram = pd.concat(data_ram, ignore_index=True)

    print(merged_big_data_cpu.shape, merged_big_data_ram.shape)
    # (90090, 7) (90090, 9)

    # 0. Divide by 2 the number of failures (each failure level corresponds to 2 containers)
    merged_big_data_cpu["cpufailures"] = merged_big_data_cpu["cpufailures"] // 2
    merged_big_data_ram["ramfailures"] = merged_big_data_ram["ramfailures"] // 2

    # attribute types
    merged_big_data_cpu = merged_big_data_cpu.astype(cpu_headers)
    merged_big_data_ram = merged_big_data_ram.astype(ram_headers)

    # following https://www.digitalocean.com/community/tutorials/exploratory-data-analysis-python

    # 1. Basic Information

    print("\n---- INFO ----")
    print("CPU:")
    print(merged_big_data_cpu.info())
    print("\nRAM:")
    print(merged_big_data_ram.info())

    print("\n---- DESCRIPTION ----")
    print("CPU:\n", merged_big_data_cpu.describe())
    print("\nRAM:\n", merged_big_data_ram.describe())

    #   ---- DESCRIPTION ----
    #   CPU:
    #                 cpu  numcontainers      baseips  ipsavailable       ipscap  apparentips  cpufailures
    #   count    90090.0        90090.0      90090.0       90090.0      90090.0      90090.0      90090.0
    #   mean   17.614404       7.298912   755.227653   8658.772347       9414.0  1430.991764     0.144655
    #   std     8.116511       2.488022   406.037958   4828.979276  5018.971237   686.680416     0.425868
    #   min          0.0            0.0          0.0   2249.415044       4029.0          0.0          0.0
    #   25%    11.764706            6.0   458.937091   3675.184072       4029.0        924.0          0.0
    #   50%    16.207496            7.0   682.406998   7397.433305       8102.0       1304.0          0.0
    #   75%    22.154409            9.0   979.996389  14807.924211      16111.0       1825.0          0.0
    #   max    65.053363           20.0  3439.983182       16111.0      16111.0       5678.0          3.0
    #   
    #   RAM:
    #                 ram  numcontainers       ram_s      ram_r      ram_w  ramavailable_s  ramavailable_r  ramavailable_w  ramfailures
    #   count    90090.0        90090.0     90090.0    90090.0    90090.0         90090.0         90090.0         90090.0      90090.0
    #   mean      4.0001       7.298912  506.462041   1.391615   1.139732    18105.204626      368.121718      256.110268     0.144367
    #   std     3.672401       2.488022  502.913795   4.792341   4.561932    12171.185337        8.151337       43.073732     0.424373
    #   min          0.0            0.0         0.0        0.0        0.0       2852.3406      290.007467      197.101333          0.0
    #   25%     1.217631            6.0  186.818967     0.0152     0.0092     4129.343833        359.9776      199.991867          0.0
    #   50%     2.901928            7.0    333.4441      0.038   0.030467    16799.478733        371.9444        266.7152          0.0
    #   75%     5.695342            9.0  604.677533   0.229067   0.196667    33291.646067      375.535067      304.739167          0.0
    #   max    33.589276           20.0   4966.5762  74.025333  69.648667         34360.0          376.54           305.0          3.0


    # 2. Duplicate Values
    print(
        f"\n---- DUPLICATES:\tCPU: {merged_big_data_cpu.duplicated().sum()}\tRAM: {merged_big_data_ram.duplicated().sum()}"
    )
    #   ---- DUPLICATES:        CPU: 279        RAM: 274

    # 2.1. Drop duplicates
    merged_big_data_cpu.drop_duplicates(inplace=True)
    merged_big_data_ram.drop_duplicates(inplace=True)

    # 5. Missing Values
    print(
        f"\n---- MISSING VALUES ----\nCPU:\n{merged_big_data_cpu.isnull().sum()}\nRAM:\n{merged_big_data_ram.isnull().sum()}"
    )
    #   None

    # Count number of failures
    print(
        f"\n---- CPU FAILURES ----\n{merged_big_data_cpu['cpufailures'].value_counts()}"
    )
    #   0    79173
    #   1     8446
    #   2     1990
    #   3      202

    print(
        f"\n---- RAM FAILURES ----\n{merged_big_data_ram['ramfailures'].value_counts()}"
    )
    #   0    79176
    #   1     8456
    #   2     2002
    #   3      182


    """

    # 10. Correlation Matrix
    plt.figure()
    _, ax = plt.subplots(figsize=(10, 9), tight_layout=True)
    corr = merged_big_data_cpu.corr()
    sns.heatmap(corr, annot=True, fmt=".3f", ax=ax)
    plt.savefig(f"{big_analysis_path}correlation_matrix_cpu.png")
    plt.savefig(f"{big_analysis_path}correlation_matrix_cpu.svg")

    # Correlation Matrix shows that there is no strong correlation between cpufailures and [numcontainers, baseips, ipsavailable, ipscap, host_ltype]
    # With this information, we will try to predict cpufailures using all the features and compare it to the results of using only the features that have a correlation with cpufailures
    #   wich are [cpu, apparentips]

    plt.figure()
    _, ax = plt.subplots(figsize=(10, 9), tight_layout=True)
    corr = merged_big_data_ram.corr()
    sns.heatmap(corr, annot=True, fmt=".3f", ax=ax)
    plt.savefig(f"{big_analysis_path}correlation_matrix_ram.png")
    plt.savefig(f"{big_analysis_path}correlation_matrix_ram.svg")

    # Correlation Matrix shows no strong correlation between ramfailures and others
    plt.close('all')

    # plot every feature against cpufailures
    for feature in merged_big_data_cpu.columns:
        if feature != 'cpufailures':
            # 2 subplots:
                # 1. scatter plot
                # 2. box plot

            plt.figure()
            fig, ax = plt.subplots(1,2, figsize=(10, 5), tight_layout=True)


            # 1. scatter plot
            sns.scatterplot(x='cpufailures', y=feature, data=merged_big_data_cpu, ax=ax[0])

            # 2. box plot
            sns.boxplot(x='cpufailures', y=feature, data=merged_big_data_cpu, ax=ax[1])

            plt.savefig(f'{big_analysis_path}pairs/cpu/{feature}_vs_numfailures.png')
            plt.savefig(f'{big_analysis_path}pairs/cpu/{feature}_vs_numfailures.svg')

    for feature in merged_big_data_ram.columns:
        if feature != 'ramfailures':
            # 2 subplots:
                # 1. scatter plot
                # 2. box plot

            plt.figure()
            fig, ax = plt.subplots(1,2, figsize=(10, 5), tight_layout=True)


            # 1. scatter plot
            sns.scatterplot(x='ramfailures', y=feature, data=merged_big_data_ram, ax=ax[0])

            # 2. box plot
            sns.boxplot(x='ramfailures', y=feature, data=merged_big_data_ram, ax=ax[1])

            plt.savefig(f'{big_analysis_path}pairs/ram/{feature}_vs_numfailures.png')
            plt.savefig(f'{big_analysis_path}pairs/ram/{feature}_vs_numfailures.svg')

    plt.close('all')

    
    # Pairplot
    plt.figure()
    sns.pairplot(merged_big_data_cpu, hue='cpufailures')
    plt.savefig(f'{big_analysis_path}pairs/pairplot_cpu.png')
    plt.savefig(f'{big_analysis_path}pairs/pairplot_cpu.svg')

    plt.figure()
    sns.pairplot(merged_big_data_ram, hue='ramfailures')
    plt.savefig(f'{big_analysis_path}pairs/pairplot_ram.png')
    plt.savefig(f'{big_analysis_path}pairs/pairplot_ram.svg')
    plt.close('all')

    """
    # FEATURE SELECTION - CPU

    # select k best features
    # https://www.simplilearn.com/tutorials/machine-learning-tutorial/feature-selection-in-machine-learning
    # the aforementioned tutorial mentions that, for numerical input and categorical output, we should use ANOVA Correlation Coefficient (linear) or Kendall's rank coefficient (non-linear)

    '''
    print("\n---- SELECT K BEST FEATURES - CPU ----")

    print("\n\t-- ANOVA --")

    best_features = SelectKBest(score_func=f_classif, k="all")

    fit = best_features.fit(
        merged_big_data_cpu.drop(columns=["cpufailures"]),
        merged_big_data_cpu["cpufailures"]
    )

    dfscores = pd.DataFrame(fit.scores_)
    dfcolumns = pd.DataFrame(
        merged_big_data_cpu.drop(columns=["cpufailures"]).columns
    )

    featureScores = pd.concat([dfcolumns, dfscores], axis=1)
    featureScores.columns = ["Specs", "Score"]

    print(featureScores.sort_values(by="Score", ascending=False))

    #              Specs        Score
    #   0            cpu  1657.570152
    #   5    apparentips  1639.296489
    #   1  numcontainers     4.107048
    #   2        baseips     2.212147
    #   3   ipsavailable     0.710469
    #   4         ipscap     0.541987

    """
    print('\n\t-- KENDALL --')

    best_features = SelectKBest(score_func=kendalltau, k='all')

    fit = best_features.fit(merged_big_data.drop(columns=['cpufailures', 'ramfailures']), merged_big_data['ramfailures'])

    dfscores = pd.DataFrame(fit.scores_)
    dfcolumns = pd.DataFrame(merged_big_data.drop(columns=['cpufailures', 'ramfailures']).columns)

    featureScores = pd.concat([dfcolumns, dfscores], axis=1)
    featureScores.columns = ['Specs', 'Score']

    print(featureScores.sort_values(by='Score', ascending=False))
    """

    print("\n\t-- CHI2 --")

    best_features = SelectKBest(score_func=chi2, k="all")

    fit = best_features.fit(
        merged_big_data_cpu.drop(columns=["cpufailures"]),
        merged_big_data_cpu["cpufailures"].astype("category")
    )

    dfscores = pd.DataFrame(fit.scores_)
    dfcolumns = pd.DataFrame(
        merged_big_data_cpu.drop(columns=["cpufailures"]).columns
    )

    featureScores = pd.concat([dfcolumns, dfscores], axis=1)
    featureScores.columns = ["Specs", "Score"]

    print(featureScores.sort_values(by="Score", ascending=False))

    #              Specs         Score
    #   5    apparentips  1.516488e+06
    #   0            cpu  1.737074e+04
    #   3   ipsavailable  5.738748e+03
    #   4         ipscap  4.353412e+03
    #   2        baseips  1.433564e+03
    #   1  numcontainers  1.017751e+01


    # FEATURE IMPORTANCE - CPU
    print("\n---- FEATURE IMPORTANCE ----")

    model = ExtraTreesClassifier()
    model.fit(
        merged_big_data_cpu.drop(columns=["cpufailures"]),
        merged_big_data_cpu["cpufailures"],
    )

    print(model.feature_importances_)

    feat_importances = pd.Series(
        model.feature_importances_,
        merged_big_data_cpu.drop(columns=["cpufailures"]).columns
    )
    print(feat_importances.sort_values(ascending=False))

    #   numcontainers    0.296458
    #   cpu              0.210491
    #   apparentips      0.207965
    #   ipsavailable     0.139228
    #   baseips          0.136735
    #   ipscap           0.009123

    '''

    """
    # PCA
    print("\n---- PCA ----")

    pca = PCA(n_components=2)
    principalComponents = pca.fit_transform(
        merged_big_data_cpu.drop(columns=["cpufailures"])
    )
    principalDf = pd.DataFrame(
        data=principalComponents,
        columns=["principal component 1", "principal component 2"],
    )

    finalDf = pd.concat(
        [principalDf, merged_big_data_cpu[["cpufailures"]]], axis=1
    )

    print(pca.explained_variance_ratio_)
    # [0.99000531 0.00887902]

    plt.figure(figsize=(8, 6))
    sns.scatterplot(
        x="principal component 1",
        y="principal component 2",
        hue="cpufailures",
        data=finalDf,
        s=100,
        alpha=0.75,
    )

    plt.savefig(f"{big_analysis_path}dim_red/pca_cpu.png")
    plt.savefig(f"{big_analysis_path}dim_red/pca_cpu.svg")

    # TSNE
    # it takes a lot of time to run (~650s)
    print("\n---- TSNE ----")

    tsne = TSNE(n_components=2)
    tsne_results = tsne.fit_transform(
        merged_big_data_cpu.drop(columns=["cpufailures"])
    )
    
    tsneDf = pd.DataFrame(
        data=tsne_results, columns=["tsne component 1", "tsne component 2"]
    )

    finalDf = pd.concat(
        [tsneDf, merged_big_data_cpu[["cpufailures"]]], axis=1
    )

    plt.figure(figsize=(8, 6))
    sns.scatterplot(
        x="tsne component 1",
        y="tsne component 2",
        hue="cpufailures",
        data=finalDf,
        s=100,
        alpha=0.75,
    )

    plt.savefig(f"{big_analysis_path}dim_red/tsne_cpu.png")
    plt.savefig(f"{big_analysis_path}dim_red/tsne_cpu.svg")

    """

    """
    # AI

    # Train and Evaluate with all features - CPU
    metrics, _ = train_and_evaluate(
        merged_big_data_cpu,
        "cpufailures",
        RandomForestClassifier(n_estimators=100, n_jobs=-1),
        binary=False,
    )
    # binary classification
    metrics_bin, _ = train_and_evaluate(
        merged_big_data_cpu,
        "cpufailures",
        RandomForestClassifier(n_estimators=100, n_jobs=-1),
        binary=True,
    )

    print(
        f'''\t{'METRICS ALL FEATURES':<48}\t{'METRICS ALL FEATURES (binary)':<48}
        \t{'accuracy':<10}{'precision':<10}{'recall':<10}{'f1':<10}\t\t{'accuracy':<10}{'precision':<10}{'recall':<10}{'f1':<10}
        mean\t{np.mean(metrics[0]):<10.4f}{np.mean(metrics[1]):<10.4f}{np.mean(metrics[2]):<10.4f}{np.mean(metrics[3]):<10.4f}\t\t{np.mean(metrics_bin[0]):<10.4f}{np.mean(metrics_bin[1]):<10.4f}{np.mean(metrics_bin[2]):<10.4f}{np.mean(metrics_bin[3]):<10.4f}
        median\t{np.median(metrics[0]):<10.4f}{np.median(metrics[1]):<10.4f}{np.median(metrics[2]):<10.4f}{np.median(metrics[3]):<10.4f}\t\t{np.median(metrics_bin[0]):<10.4f}{np.median(metrics_bin[1]):<10.4f}{np.median(metrics_bin[2]):<10.4f}{np.median(metrics_bin[3]):<10.4f}
        std\t{np.std(metrics[0]):<10.4f}{np.std(metrics[1]):<10.4f}{np.std(metrics[2]):<10.4f}{np.std(metrics[3]):<10.4f}\t\t{np.std(metrics_bin[0]):<10.4f}{np.std(metrics_bin[1]):<10.4f}{np.std(metrics_bin[2]):<10.4f}{np.std(metrics_bin[3]):<10.4f}
        '''\
    )

    #   METRICS ALL FEATURES                                METRICS ALL FEATURES (binary)                   
    #           accuracy  precision recall    f1        		accuracy  precision recall    f1        
    #   mean	0.9318    0.7297    0.5922    0.6384    		0.9459    0.8306    0.6780    0.7466    
    #   median	0.9317    0.7359    0.5920    0.6388    		0.9460    0.8302    0.6773    0.7462    
    #   std	    0.0012    0.0240    0.0119    0.0145    		0.0010    0.0074    0.0069    0.0051    
       

    # Train and Evaluate with all features - RAM
    metrics, _ = train_and_evaluate(
        merged_big_data_ram,
        "ramfailures",
        RandomForestClassifier(n_estimators=100, n_jobs=-1),
        binary=False,
    )
    # binary classification
    metrics_bin, _ = train_and_evaluate(
        merged_big_data_ram,
        "ramfailures",
        RandomForestClassifier(n_estimators=100, n_jobs=-1),
        binary=True,
    )

    print(
        f'''\t{'METRICS ALL FEATURES':<48}\t{'METRICS ALL FEATURES (binary)':<48}
        \t{'accuracy':<10}{'precision':<10}{'recall':<10}{'f1':<10}\t\t{'accuracy':<10}{'precision':<10}{'recall':<10}{'f1':<10}
        mean\t{np.mean(metrics[0]):<10.4f}{np.mean(metrics[1]):<10.4f}{np.mean(metrics[2]):<10.4f}{np.mean(metrics[3]):<10.4f}\t\t{np.mean(metrics_bin[0]):<10.4f}{np.mean(metrics_bin[1]):<10.4f}{np.mean(metrics_bin[2]):<10.4f}{np.mean(metrics_bin[3]):<10.4f}
        median\t{np.median(metrics[0]):<10.4f}{np.median(metrics[1]):<10.4f}{np.median(metrics[2]):<10.4f}{np.median(metrics[3]):<10.4f}\t\t{np.median(metrics_bin[0]):<10.4f}{np.median(metrics_bin[1]):<10.4f}{np.median(metrics_bin[2]):<10.4f}{np.median(metrics_bin[3]):<10.4f}
        std\t{np.std(metrics[0]):<10.4f}{np.std(metrics[1]):<10.4f}{np.std(metrics[2]):<10.4f}{np.std(metrics[3]):<10.4f}\t\t{np.std(metrics_bin[0]):<10.4f}{np.std(metrics_bin[1]):<10.4f}{np.std(metrics_bin[2]):<10.4f}{np.std(metrics_bin[3]):<10.4f}
        '''\
    )

    #   METRICS ALL FEATURES                            	METRICS ALL FEATURES (binary)                   
    #   	    accuracy  precision recall    f1        		accuracy  precision recall    f1        
    #   mean	0.8801    0.2511    0.2500    0.2355    		0.8793    0.1341    0.0044    0.0085    
    #   median	0.8801    0.2457    0.2500    0.2353    		0.8794    0.1299    0.0042    0.0082    
    #   std	    0.0014    0.0180    0.0003    0.0006    		0.0015    0.0316    0.0011    0.0022    
        
    """

    
    # CPU Random Forest Classifier with grid search
    # Use f1 as scoring metric

    param_grid = {
        "n_estimators": [50, 100, 200, 500],                  # default 100
        "criterion": ["gini", "entropy", "log_loss"],               # default "gini"
        "min_samples_split": [2, 5, 10],                            # default 2
        "min_samples_leaf": [1, 2, 5],                              # default 1
        "max_features": [None, "sqrt", "log2"],                     # default "sqrt"
        "bootstrap": [True, False],                                 # default True
        "n_jobs": [-1],
    }

    # Train and Evaluate with all features - CPU

    #metrics, _ = train_and_evaluate(
    #    merged_big_data_cpu,
    #    "cpufailures",
    #    RandomForestClassifier(),
    #    binary=False,
    #    grid_search=True,
    #    param_grid=param_grid,
    #)
    
    """
    #   Grid search time: 35103.148278713226
    #   {'bootstrap': True, 'criterion': 'entropy', 'max_features': None, 'min_samples_leaf': 5, 'min_samples_split': 10, 'n_estimators': 500, 'n_jobs': -1}
    #   RandomForestClassifier(criterion='entropy', max_features=None,
    #                          min_samples_leaf=5, min_samples_split=10,
    #                          n_estimators=500, n_jobs=-1)
    #   Train data
    #                 precision    recall  f1-score   support
    #   
    #              0       0.97      0.99      0.98     55433
    #              1       0.86      0.67      0.75      5901
    #              2       0.85      0.76      0.80      1392
    #              3       0.88      0.43      0.58       141
    #   
    #       accuracy                           0.96     62867
    #      macro avg       0.89      0.71      0.78     62867
    #   weighted avg       0.95      0.96      0.95     62867
    #   
    #   Test data
    #                 precision    recall  f1-score   support
    #   
    #              0       0.96      0.99      0.97     23740
    #              1       0.73      0.58      0.64      2545
    #              2       0.68      0.59      0.63       598
    #              3       0.71      0.28      0.40        61
    #   
    #       accuracy                           0.94     26944
    #      macro avg       0.77      0.61      0.66     26944
    #   weighted avg       0.93      0.94      0.93     26944
    
    #   Grid search f1
    #   {'bootstrap': True, 'criterion': 'entropy', 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200, 'n_jobs': -1}
    #   RandomForestClassifier(criterion='entropy', max_features=None,
    #                          min_samples_split=10, n_estimators=200, n_jobs=-1)
    #   Train data
    #                 precision    recall  f1-score   support
    #   
    #              0       0.98      1.00      0.99     55433
    #              1       0.91      0.76      0.83      5901
    #              2       0.90      0.82      0.86      1392
    #              3       0.89      0.63      0.74       141
    #   
    #       accuracy                           0.97     62867
    #      macro avg       0.92      0.80      0.85     62867
    #   weighted avg       0.97      0.97      0.97     62867
    #   
    #   Test data
    #                 precision    recall  f1-score   support
    #   
    #              0       0.96      0.98      0.97     23740
    #              1       0.71      0.58      0.64      2545
    #              2       0.69      0.59      0.64       598
    #              3       0.74      0.46      0.57        61
    #   
    #       accuracy                           0.94     26944
    #      macro avg       0.78      0.65      0.70     26944
    #   weighted avg       0.93      0.94      0.93     26944
    """

    # binary classification
    #metrics_bin, _ = train_and_evaluate(
    #    merged_big_data_cpu,
    #    "cpufailures",
    #    RandomForestClassifier(),
    #    binary=True,
    #    grid_search=True,
    #    param_grid=param_grid,
    #)

    #   Grid search time: 5368.719045162201
    #   {'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_leaf': 5, 'min_samples_split': 10, 'n_estimators': 100, 'n_jobs': -1}
    #   RandomForestClassifier(min_samples_leaf=5, min_samples_split=10, n_jobs=-1)
    #   Train data
    #                 precision    recall  f1-score   support
    #   
    #              0       0.97      0.99      0.98     55401
    #              1       0.93      0.74      0.82      7466
    #   
    #       accuracy                           0.96     62867
    #      macro avg       0.95      0.87      0.90     62867
    #   weighted avg       0.96      0.96      0.96     62867
    #   
    #   Test data
    #                 precision    recall  f1-score   support
    #   
    #              0       0.96      0.99      0.97     23772
    #              1       0.87      0.67      0.76      3172
    #   
    #       accuracy                           0.95     26944
    #      macro avg       0.91      0.83      0.86     26944
    #   weighted avg       0.95      0.95      0.95     26944

    #   Grid search f1 : 5386.9168066978455
    #   {'criterion': 'log_loss', 'max_features': None, 'min_samples_leaf': 5, 'min_samples_split': 5, 'n_estimators': 500, 'n_jobs': -1}
    #   RandomForestClassifier(criterion='log_loss', max_features=None,
    #                          min_samples_leaf=5, min_samples_split=5,
    #                          n_estimators=500, n_jobs=-1)
    #   Train data
    #                 precision    recall  f1-score   support
    #   
    #              0       0.97      0.99      0.98     55401
    #              1       0.93      0.76      0.84      7466
    #   
    #       accuracy                           0.96     62867
    #      macro avg       0.95      0.88      0.91     62867
    #   weighted avg       0.96      0.96      0.96     62867
    #   
    #   Test data
    #                 precision    recall  f1-score   support
    #   
    #              0       0.96      0.98      0.97     23772
    #              1       0.86      0.69      0.77      3172
    #   
    #       accuracy                           0.95     26944
    #      macro avg       0.91      0.84      0.87     26944
    #   weighted avg       0.95      0.95      0.95     26944


    # RAAAAAAAAAAM

    # Train and Evaluate with all features - RAM
    metrics_bin, _ = train_and_evaluate(
        merged_big_data_ram,
        "ramfailures",
        RandomForestClassifier(),
        binary=False,
        grid_search=True,
        param_grid=param_grid,
    )

    #   Grid search time: 53953.81897568703
    #   {'bootstrap': True, 'criterion': 'log_loss', 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 200, 'n_jobs': -1}
    #   RandomForestClassifier(criterion='log_loss', min_samples_leaf=2,
    #                          min_samples_split=10, n_estimators=200, n_jobs=-1)
    #   Train data
    #                 precision    recall  f1-score   support
    #   
    #              0       0.88      1.00      0.94     55419
    #              1       1.00      0.04      0.07      5923
    #              2       0.00      0.00      0.00      1393
    #              3       0.00      0.00      0.00       136
    #   
    #       accuracy                           0.88     62871
    #      macro avg       0.47      0.26      0.25     62871
    #   weighted avg       0.87      0.88      0.83     62871
    #   
    #   Test data
    #                 precision    recall  f1-score   support
    #   
    #              0       0.88      1.00      0.94     23757
    #              1       0.00      0.00      0.00      2533
    #              2       0.00      0.00      0.00       609
    #              3       0.00      0.00      0.00        46
    #   
    #       accuracy                           0.88     26945
    #      macro avg       0.22      0.25      0.23     26945
    #   weighted avg       0.78      0.88      0.83     26945

    #   Grid search f1 : 53972.76392650604
    #   {'bootstrap': False, 'criterion': 'entropy', 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100, 'n_jobs': -1}
    #   RandomForestClassifier(bootstrap=False, criterion='entropy', max_features=None,
    #                          n_jobs=-1)
    #   Train data
    #                 precision    recall  f1-score   support
    #   
    #              0       1.00      1.00      1.00     55419
    #              1       1.00      1.00      1.00      5923
    #              2       1.00      1.00      1.00      1393
    #              3       1.00      1.00      1.00       136
    #   
    #       accuracy                           1.00     62871
    #      macro avg       1.00      1.00      1.00     62871
    #   weighted avg       1.00      1.00      1.00     62871
    #   
    #   Test data
    #                 precision    recall  f1-score   support
    #   
    #              0       0.88      0.88      0.88     23757
    #              1       0.11      0.11      0.11      2533
    #              2       0.02      0.02      0.02       609
    #              3       0.02      0.02      0.02        46
    #   
    #       accuracy                           0.78     26945
    #      macro avg       0.26      0.26      0.26     26945
    #   weighted avg       0.79      0.78      0.79     26945

    # binary classification
    metrics_bin, _ = train_and_evaluate(
        merged_big_data_ram,
        "ramfailures",
        RandomForestClassifier(),
        binary=True,
        grid_search=True,
        param_grid=param_grid,
    )

    #   Grid search time: 56693.08396792412
    #   {'bootstrap': True, 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_leaf': 5, 'min_samples_split': 2, 'n_estimators': 200, 'n_jobs': -1}
    #   RandomForestClassifier(criterion='entropy', min_samples_leaf=5,
    #                          n_estimators=200, n_jobs=-1)
    #   Train data
    #                 precision    recall  f1-score   support
    #   
    #              0       0.88      1.00      0.94     55431
    #              1       1.00      0.01      0.01      7440
    #   
    #       accuracy                           0.88     62871
    #      macro avg       0.94      0.50      0.48     62871
    #   weighted avg       0.90      0.88      0.83     62871
    #   
    #   Test data
    #                 precision    recall  f1-score   support
    #   
    #              0       0.88      1.00      0.94     23745
    #              1       0.00      0.00      0.00      3200
    #   
    #       accuracy                           0.88     26945
    #      macro avg       0.44      0.50      0.47     26945
    #   weighted avg       0.78      0.88      0.83     26945

    #   Grid search f1 : 56717.54580140114
    #   {'bootstrap': False, 'criterion': 'log_loss', 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50, 'n_jobs': -1}
    #   RandomForestClassifier(bootstrap=False, criterion='log_loss', max_features=None,
    #                          n_estimators=50, n_jobs=-1)
    #   Train data
    #                 precision    recall  f1-score   support
    #   
    #              0       1.00      1.00      1.00     55431
    #              1       1.00      1.00      1.00      7440
    #   
    #       accuracy                           1.00     62871
    #      macro avg       1.00      1.00      1.00     62871
    #   weighted avg       1.00      1.00      1.00     62871
    #   
    #   Test data
    #                 precision    recall  f1-score   support
    #   
    #              0       0.88      0.88      0.88     23745
    #              1       0.12      0.13      0.12      3200
    #   
    #       accuracy                           0.79     26945
    #      macro avg       0.50      0.50      0.50     26945
    #   weighted avg       0.79      0.79      0.79     26945


def test():
    # plot ram from datasets

    # read data 1
    datapath = (
        "logs/MyFog_MyAzure2019Workload_100_6_30_10000_300_1/hostinfo_with_interval.csv"
    )
    data = pd.read_csv(datapath)

    num_hosts = 3

    headers = [
        "ram_s",
        "ram_r",
        "ram_w",
        "ramavailable_s",
        "ramavailable_r",
        "ramavailable_w",
    ]

    data = data[["interval"] + headers]

    hosts = [data.copy() for _ in range(num_hosts)]

    for i in range(num_hosts):
        for h in headers:
            hosts[i][h] = hosts[i][h].apply(lambda x: ast.literal_eval(x)[i])

        print(hosts[i].head())
        print(hosts[i].describe())

    # plot with 'interval' as x axis

    # polot horizontal line in each plot
    list_ram = [[4295, 17180, 34360], [372.0, 360.0, 376.54], [200.0, 305.0, 266.75]]

    _, ax = plt.subplots(3, 3, figsize=(15, 10))
    for i, host in enumerate(hosts):
        for r in ["ram", "ramavailable"]:
            for j, hh in enumerate(["s", "r", "w"]):
                ax[i][j].axhline(y=list_ram[j][i], color="r", linestyle="-")
                sns.lineplot(x="interval", y=f"{r}_{hh}", data=host, ax=ax[i][j])

    plt.savefig("ram.png")
    plt.savefig("ram.svg")


def train_and_evaluate(data, y_col, model, data_test=None, binary=False, grid_search=False, param_grid=None):
    """
    Train and evaluate a model using the given data and model
    It will run NUMBER_OF_REPETITIONS times

    Parameters
    ----------
    data : pandas.DataFrame
        Data to train and evaluate the model
    y_col : str
        Name of the column to predict
    model : sklearn.model
        Model to train and evaluate
    data_test : pandas.DataFrame, optional
        Test data to evaluate the model, by default None
    binary : bool, optional
        If True, the data will be converted to binary, by default False
    grid_search : bool, optional
        If True, the model will be trained using grid search, by default False
    param_grid : dict, optional
        Dictionary with the parameters to use in grid search, by default None
        When grid_search is True, param_grid must be provided

    Returns
    -------
    list
        List of metrics [accuracy, precision, recall, f1]
    tuple
        Tuple with the best predicted values and respective f1 score
    """

    if binary:
        data[y_col] = data[y_col].apply(lambda x: 1 if x > 0 else 0)

    if grid_search:
        if not param_grid:
            print("param_grid must be provided for grid search")
            return
        
        # split data if test data is not provided
        if data_test is None:
            train, test = train_test_split(data, test_size=0.3, shuffle=True)
        else:
            train = data
            test = data_test

        x_train = train.drop(columns=[y_col])
        y_train = train[y_col]

        x_test = test.drop(columns=[y_col])
        y_test = test[y_col]

        ## NORMAL GRID SEARCH

        t = time.time()

        grid = GridSearchCV(model, param_grid, verbose=2, n_jobs=-1)

        grid.fit(x_train, y_train)

        print(f"Grid search time: {time.time() - t}")

        print(grid.best_params_)
        print(grid.best_estimator_)

        # evaluate train data and test data

        print("Train data")
        y_pred = grid.predict(x_train)

        print(classification_report(y_train, y_pred))

        print("Test data")

        y_pred = grid.predict(x_test)

        print(classification_report(y_test, y_pred))

        ## GRID SEARCH WITH F1 SCORING

        t = time.time()

        grid = GridSearchCV(model, param_grid, verbose=2, n_jobs=-1, scoring=make_scorer(f1_score , average="binary" if binary else "macro"))

        grid.fit(x_train, y_train)

        print(f"Grid search f1 : {time.time() - t}")

        print(grid.best_params_)
        print(grid.best_estimator_)

        # evaluate train data and test data

        print("Train data")
        y_pred = grid.predict(x_train)

        print(classification_report(y_train, y_pred))

        print("Test data")

        y_pred = grid.predict(x_test)

        print(classification_report(y_test, y_pred))


        
        return None, None

        

    metrics = [[], [], [], []]
    best_f1 = 0
    y_pred_best = None
    for _ in range(NUMBER_OF_REPETITIONS):
        # split data if test data is not provided
        if data_test is None:
            train, test = train_test_split(data, test_size=0.3, shuffle=True)
        else:
            train = data
            test = data_test

        x_train = train.drop(columns=[y_col])
        y_train = train[y_col]

        x_test = test.drop(columns=[y_col])
        y_test = test[y_col]

        # train and predict
        model.fit(x_train, y_train)
        y_pred = model.predict(x_test)

        # evaluate
        metrics[0].append(accuracy_score(y_test, y_pred))
        metrics[1].append(
            precision_score(y_test, y_pred, average="binary" if binary else "macro")
        )
        metrics[2].append(
            recall_score(y_test, y_pred, average="binary" if binary else "macro")
        )
        metrics[3].append(
            f1_score(y_test, y_pred, average="binary" if binary else "macro")
        )

        # Why macro? https://towardsdatascience.com/micro-macro-weighted-averages-of-f1-score-clearly-explained-b603420b292f
        # Because the classes are imbalanced, so we want to give the same importance to each class. average='weighted' would give more importance to the majority class

        # copilot sugested the following link: https://datascience.stackexchange.com/questions/15989/micro-average-vs-macro-average-performance-in-a-multiclass-classification-settin

        if metrics[3][-1] > best_f1:
            best_f1 = metrics[3][-1]
            y_pred_best = y_pred

    return metrics, (y_pred_best, best_f1)


def plot_metrics(metrics, name):
    """
    Plot the metrics

    Parameters
    ----------
    metrics : list
        List of metrics to plot [accuracy, precision, recall, f1]
    name : str
        Name of the file to save the plot
    """

    plt.figure()
    plt.boxplot(metrics)
    plt.xticks([1, 2, 3, 4], ["Accuracy", "Precision", "Recall", "F1"])
    plt.savefig(f"{FIGURES_PATH}metrics/{name}.png")
    plt.savefig(f"{FIGURES_PATH}metrics/{name}.svg")


if __name__ == "__main__":
    time_start = time.time()

    # generate_datasets()

    # failure_distribution()

    # plot_data()

    # train_and_evaluate_big_data()

    big_merged_data_eda()

    # test()

    print(f"Time taken: {time.time() - time_start}")
